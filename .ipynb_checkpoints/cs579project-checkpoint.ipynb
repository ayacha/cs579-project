{
 "metadata": {
  "name": "",
  "signature": "sha256:1bdd4749626a7463a14886891cd902a0cf237769e1fc93ff8924b13a70218e66"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.Collect the data have \"#TheWalkingDead\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ConfigParser\n",
      "from TwitterAPI import TwitterAPI,TwitterRestPager\n",
      "import sys\n",
      "import time\n",
      "import ast,re\n",
      "\n",
      "def get_twitter(config_file):\n",
      "    config = ConfigParser.ConfigParser()\n",
      "    config.read(config_file)\n",
      "    twitter = TwitterAPI(\n",
      "                   config.get('twitter', 'consumer_key'),\n",
      "                   config.get('twitter', 'consumer_secret'),\n",
      "                   config.get('twitter', 'access_token'),\n",
      "                   config.get('twitter', 'access_token_secret'))\n",
      "    return twitter\n",
      "twitter = get_twitter('twitter1.cfg')\n",
      "print 'Established Twitter connection.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Established Twitter connection.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data(string,filename):\n",
      "\tpager = TwitterRestPager(twitter,'search/tweets',{'q':'#TheWalkingDead','count':200})\n",
      "\ti=0\n",
      "\tf=open(filename,'w')\n",
      "\tfor item in pager.get_iterator(new_tweets=True):\n",
      "\t\tf.write(str(item)+'\\n')\n",
      "\t\ti=i+1\n",
      "\t\tprint i\n",
      "\tf.close\n",
      "\n",
      "\n",
      "\n",
      "# get_data('#TheWalkingDead','TheWalkingDead2.txt')\n",
      "print 'ok'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_date(filename):\n",
      "\tcount=0\n",
      "\tf=open(filename,'r')\n",
      "\tfor i in f:\n",
      "\t\ti=ast.literal_eval(i)\n",
      "\t\tif i.has_key('created_at'):\n",
      "\t\t\tcount=count+1\n",
      "\t\t\tprint i['created_at']\n",
      "\t\t\tprint count\n",
      "\t\telse:\n",
      "\t\t\ti.clear()\n",
      "# get_date('TheWalkingDead.txt')\n",
      "# get_date('TheWalkingDead20.txt')\n",
      "# get_date('TheWalkingDead26.txt')\n",
      "# total have 414,700 data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Get data from txt file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# return a list of tuple that have user name, user text and user description\n",
      "import ast\n",
      "def get_des(filename):\n",
      "    f=open(filename,'r')\n",
      "    info=[]\n",
      "    for i in f:\n",
      "        i=ast.literal_eval(i)\n",
      "        if i['lang']==u'en':\n",
      "            info.append((i['user']['name'].encode('utf-8'),i['text'].encode('utf-8'),i['user']['description'].encode('utf-8')))\n",
      "    f.close()\n",
      "    return info\n",
      "# a=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead20.txt')\n",
      "# b=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead21-25.txt')\n",
      "# c=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead26.txt')\n",
      "# d=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead27-03.txt')\n",
      "info_pos=get_des('/Users/zhaoyixuan/Documents/cs579/positive.txt')\n",
      "b=get_des('/Users/zhaoyixuan/Documents/cs579/negative.txt')\n",
      "c=get_des('/Users/zhaoyixuan/Documents/cs579/negativefuck.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(info_pos),len(b),len(c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2905 394 1364\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add all the data into info list\n",
      "info_neg=[]\n",
      "count=0\n",
      "g=[b,c]\n",
      "for i in g:\n",
      "    for x in i:\n",
      "        info_neg.append(x)\n",
      "        count+=1\n",
      "print count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1758\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of corresponding name, text and description\n",
      "name_test=[]\n",
      "text_test=[]\n",
      "description_test=[]\n",
      "for i in info:\n",
      "    name_test.append(i[0])\n",
      "    text_test.append(i[1])\n",
      "    description.append(i[2])\n",
      "print 'Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(info, open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'wb'))\n",
      "pickle.dump(name, open('/Users/zhaoyixuan/Documents/cs579/name_test.pkl', 'wb'))\n",
      "pickle.dump(text, open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'wb'))\n",
      "pickle.dump(description, open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'wb'))\n",
      "pickle.dump(info1,open('/Users/zhaoyixuan/Documents/cs579/info1.pkl', 'wb'))\n",
      "pickle.dump(test_tweet,open('/Users/zhaoyixuan/Documents/cs579/test_tweet.pkl', 'wb'))\n",
      "pickle.dump(test_label,open('/Users/zhaoyixuan/Documents/cs579/test_label.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'info' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-5adcad17cfb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/zhaoyixuan/Documents/cs579/info.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/zhaoyixuan/Documents/cs579/name_test.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/zhaoyixuan/Documents/cs579/text.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/zhaoyixuan/Documents/cs579/description.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'info' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "# info=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'rb'))\n",
      "name=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/name.pkl', 'rb'))\n",
      "# text=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'rb'))\n",
      "# description=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'rb'))\n",
      "info1=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/info1.pkl', 'rb'))\n",
      "test_tweet=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/test_tweet.pkl', 'rb'))\n",
      "test_label=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/test_label.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print info_pos[0]\n",
      "print info_neg[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('The Walking Dead', \"RT @IGN: Walking Dead producer delves into Rick &amp; Daryl's new dynamic. Plus, why'd Sasha do THAT? http://t.co/gzVcmnt3aY http://t.co/yIjjYF\\xe2\\x80\\xa6\", 'Sundays @ 9|8c. Only on @AMC_TV.')\n",
        "('\\xe2\\xad\\x90Starla\\xe2\\xad\\x90', \"I can't believe there is only 1 more new The Walking Dead episode this year. #sucks\", 'Loves:  Music, travel, The Walking Dead, Game of Thrones.')\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.Filter data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fetch male/female names from Census.\n",
      "import requests\n",
      "\n",
      "def get_census_names():\n",
      "    \"\"\" Fetch a list of common male/female names from the census.\n",
      "    For ambiguous names, we select the more frequent gender.\"\"\"\n",
      "    males = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first').text.split('\\n')\n",
      "    females = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first').text.split('\\n')\n",
      "    males_pct = dict([(m.split()[0].lower(), float(m.split()[1]))\n",
      "                  for m in males if m])\n",
      "    females_pct = dict([(f.split()[0].lower(), float(f.split()[1]))\n",
      "                    for f in females if f])\n",
      "    male_names = set([m for m in males_pct if m not in females_pct or\n",
      "                  males_pct[m] > females_pct[m]])\n",
      "    female_names = set([f for f in females_pct if f not in males_pct or\n",
      "                  females_pct[f] > males_pct[f]])    \n",
      "    return male_names, female_names\n",
      "\n",
      "male_names, female_names = get_census_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "from collections import Counter\n",
      "def get_first_name(t):\n",
      "    parts =t.split()\n",
      "    if len(parts)>0:\n",
      "        return parts[0].lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter the data and keep the first name in male_names of female_names\n",
      "def sample(name_test, male_names, female_names):\n",
      "    count=0\n",
      "    real_name=[]\n",
      "    num=[]\n",
      "    i=0\n",
      "    for t in name_test:\n",
      "        name = get_first_name(t)\n",
      "        if name in male_names:\n",
      "            real_name.append(name)\n",
      "            num.append(i)\n",
      "        if name in  female_names:\n",
      "            real_name.append(name)\n",
      "            num.append(i)\n",
      "        i+=1\n",
      "    return real_name,num"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "real_name,num=sample(name,male_names,female_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# info1 store the filtered data\n",
      "info1=[]\n",
      "for i in num:\n",
      "    info1.append(info[i])\n",
      "print len(info1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "330296\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print real_name[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['chad', 'sammy', 'katie', 'abraham', 'fernando', 'tammie', 'katherine', 'angela', 'raven', 'holly', 'sam', 'tammie', 'jessica', 'kiara', 'ana', 'tatiana', 'daryl', 'dana', 'max', 'cathy']\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Sentiment analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from StringIO import StringIO\n",
      "from zipfile import ZipFile\n",
      "from urllib import urlopen\n",
      "\n",
      "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
      "zipfile = ZipFile(StringIO(url.read()))\n",
      "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
      "\n",
      "afinn = dict()\n",
      "\n",
      "for line in afinn_file:\n",
      "    parts = line.strip().split()\n",
      "    if len(parts) == 2:\n",
      "        afinn[parts[0]] = int(parts[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'read', len(afinn), 'AFINN terms.\\nE.g.:', afinn.items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 2462 AFINN terms.\n",
        "E.g.: [('limited', -1), ('suicidal', -2), ('pardon', 2), ('desirable', 2), ('protest', -2)]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def afinn_sentiment(terms, afinn):\n",
      "    total = 0.\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            total += afinn[t]\n",
      "    return total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def afinn_sentiment2(terms, afinn, verbose=False):\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            if verbose:\n",
      "                print '\\t%s=%d' % (t, afinn[t])\n",
      "            if afinn[t] > 0:\n",
      "                pos += afinn[t]\n",
      "            else:\n",
      "                neg += 1 * afinn[t]\n",
      "    return pos, neg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score=[]\n",
      "for tweet in info_pos:\n",
      "    score.append((' '.join(tweet[1].split()),4))\n",
      "for tweet in info_neg:\n",
      "    score.append((' '.join(tweet[1].split()),2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(info_pos),len(info_neg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2905 1758\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "import numpy as np\n",
      "y2=np.array([t[1] for t in score])\n",
      "print 'label counts=', Counter(y2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "label counts= Counter({4: 2905, 2: 1758})\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(t[0] for t in score)\n",
      "print 'vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vectorized 4663 tweets. found 7770 terms.\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(X, y2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# LogisticRegresstion\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "def do_cross_val_LR(C_var,X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression(C=C_var)\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C_var=[0.01,0.1,0.5,1.0,5.0,10.0,100.0,1000.0]\n",
      "result=[]\n",
      "for i in C_var:\n",
      "    avg=do_cross_val_LR(i,X,y2,5)\n",
      "    result.append(avg)\n",
      "    print 'avg accuracy', i,avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy 0.01 0.902164092939\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.1 0.908160026496\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.5 0.911590052855\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.0 0.912019237404\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.0 0.914379522423\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10.0 0.914808706972\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100.0 0.926612202089\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000.0 0.93391040945\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "C_var=[0.01,0.1,0.5,1.0,5.0,10.0,100.0,1000.0]\n",
      "result=[0.90216409293938504, 0.90816002649628091, 0.91159005285456018, 0.91201923740391633, 0.91437952242293785, 0.91480870697229411, 0.92661220208934214, 0.9339104094503401]\n",
      "plt.figure()\n",
      "print 'ok',\n",
      "# plt.title('Accuracy of Logistic Regression')\n",
      "# print 'ok1',\n",
      "plt.plot(result, 'bo')\n",
      "print 'ok2',\n",
      "plt.xlabel('C')\n",
      "print 'ok3',\n",
      "plt.ylabel('avg accuracy')\n",
      "print 'ok4',\n",
      "plt.show()\n",
      "print 'ok5',"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "clf = svm.SVC()\n",
      "clf.fit(X, y2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 494,
       "text": [
        "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 494
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# svm\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf =svm.SVC()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 495
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val(X,y2, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.603455326626\n"
       ]
      }
     ],
     "prompt_number": 496
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X=X.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 499
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "gnb = GaussianNB()\n",
      "gnb.fit(X,y2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 500,
       "text": [
        "GaussianNB()"
       ]
      }
     ],
     "prompt_number": 500
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# GaussianNB\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf =GaussianNB()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 501
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val(X,y2, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.692262027977\n"
       ]
      }
     ],
     "prompt_number": 502
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# predicate test data\n",
      "vectorizer1 = CountVectorizer()\n",
      "X2 = vectorizer1.fit_transform(t[1] for t in info1[94461:108065])\n",
      "print 'vectorized %d tweets. found %d terms.' % (X2.shape[0], X2.shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vectorized 13604 tweets. found 7770 terms.\n"
       ]
      }
     ],
     "prompt_number": 404
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from sklearn.linear_model import *\n",
      "import sklearn.linear_model\n",
      "from sklearn.svm import SVC  \n",
      "y_pred8=model.predict(X2) #You should predict on the validation data\n",
      "print Counter(y_pred8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({4: 13590, 2: 14})\n"
       ]
      }
     ],
     "prompt_number": 405
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred=y_pred.tolist()\n",
      "pred1=y_pred1.tolist()\n",
      "pred2=y_pred2.tolist()\n",
      "pred3=y_pred3.tolist()\n",
      "pred4=y_pred4.tolist()\n",
      "pred5=y_pred5.tolist()\n",
      "pred6=y_pred6.tolist()\n",
      "pred7=y_pred7.tolist()\n",
      "pred8=y_pred8.tolist()\n",
      "print len(pred),len(pred1),len(pred2),len(pred3),len(pred4),len(pred5),len(pred6),len(pred7),len(pred8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7714 22674 11091 8267 9888 6576 13031 15191 13604\n"
       ]
      }
     ],
     "prompt_number": 406
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_label=[]\n",
      "list_pred=[pred,pred1,pred2,pred3,pred4,pred5,pred6,pred7,pred8]\n",
      "for i in list_pred:\n",
      "    c=0\n",
      "    for num in i:\n",
      "        c+=1\n",
      "        test_label.append(num)\n",
      "    print c,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7714 22674 11091 8267 9888 6576 13031 15191 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13604\n"
       ]
      }
     ],
     "prompt_number": 423
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_test=[(1,7715),(7716,30390),(30391,41482),(41483,49750),(49762,59650),(59660,66236),(66237,79268),(79269,94460),(94461,108065)]\n",
      "test_tweet=[]\n",
      "real_name=[]\n",
      "for i in list_test:\n",
      "    a=i[0]\n",
      "    c=0\n",
      "    for a in range(a,i[1]):\n",
      "        test_tweet.append(info1[a])\n",
      "        real_name.append(info1[a][0])\n",
      "        c+=1\n",
      "    print c,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7714 22674 11091 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8267 9888 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6576 13031 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15191 13604\n"
       ]
      }
     ],
     "prompt_number": 480
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(test_tweet),len(test_label)\n",
      "print Counter(test_label)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "108036 108036\n",
        "Counter({4: 107847, 2: 189})"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 481
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.Gender prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(string, lowercase, keep_punctuation, prefix,\n",
      "             collapse_urls, collapse_mentions):\n",
      "    if not string:\n",
      "        return []\n",
      "    if lowercase:\n",
      "        string = string.lower()\n",
      "    tokens = []\n",
      "    if collapse_urls:\n",
      "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
      "    if collapse_mentions:\n",
      "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
      "    if keep_punctuation:\n",
      "        tokens = string.split()\n",
      "    else:\n",
      "        tokens = re.sub('\\W+', ' ', string).split()\n",
      "    if prefix:\n",
      "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
      "    return tokens\n",
      "\n",
      "def tweet2tokens(infomation, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True):\n",
      "    tokens = tokenize(infomation[1], lowercase, keep_punctuation, None,\n",
      "                       collapse_urls, collapse_mentions)\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(infomation[2], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 447
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's tokenize all tweets.\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=False, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True)\n",
      "              for t in test_tweet]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 448
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "108036\n"
       ]
      }
     ],
     "prompt_number": 449
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Store these in a sparse matrix.\n",
      "from collections import defaultdict\n",
      "\n",
      "def make_vocabulary(tokens_list):\n",
      "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
      "    for tokens in tokens_list:\n",
      "        for token in tokens:\n",
      "            vocabulary[token]\n",
      "    print '%d unique terms in vocabulary' % len(vocabulary)\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 450
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "64622 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 451
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert features to a sparse matrix X.\n",
      "# X[i,j] is the frequency of term j in tweet i\n",
      "# \n",
      "from scipy.sparse import lil_matrix\n",
      "\n",
      "def make_feature_matrix(tokens_list, vocabulary):\n",
      "    X = lil_matrix((len(test_tweet), len(vocabulary)))\n",
      "    for i, tokens in enumerate(tokens_list):\n",
      "        for token in tokens:\n",
      "            j = vocabulary[token]\n",
      "            X[i,j] += 1\n",
      "    return X.tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 452
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = make_feature_matrix(tokens_list, vocabulary)\n",
      "print 'shape of X:', X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shape of X: (108036, 64622)\n"
       ]
      }
     ],
     "prompt_number": 453
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let 1=Female, 0=Male.\n",
      "import numpy as np\n",
      "\n",
      "def get_gender(t, male_names, female_names):\n",
      "    name_label = get_first_name(t)\n",
      "    if name_label in female_names:\n",
      "        return 1\n",
      "    elif name_label in male_names:\n",
      "        return 0\n",
      "    else:\n",
      "        return -1\n",
      "    \n",
      "y = np.array([get_gender(t, male_names, female_names) for t in real_name])\n",
      "print 'gender labels:', Counter(y).items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gender labels: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 40560), (1, 67476)]\n"
       ]
      }
     ],
     "prompt_number": 483
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Do 5-fold cross-validation\n",
      "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 484
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val(X, y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.74112310916\n"
       ]
      }
     ],
     "prompt_number": 485
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tweet2tokens(info, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    tokens = []\n",
      "    if use_text:\n",
      "        tokens.extend(tokenize(info[1], lowercase, keep_punctuation, None,\n",
      "                           collapse_urls, collapse_mentions))\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(info[2], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 489
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_all(info, use_descr=True, lowercase=True,\n",
      "            keep_punctuation=True, descr_prefix=None,\n",
      "            collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    \n",
      "    tokens_list = [tweet2tokens(t, use_descr, lowercase,\n",
      "                            keep_punctuation, descr_prefix,\n",
      "                            collapse_urls, collapse_mentions, use_text)\n",
      "                  for t in info]\n",
      "    vocabulary = make_vocabulary(tokens_list)\n",
      "    X = make_feature_matrix(tokens_list, vocabulary)\n",
      "    acc = do_cross_val(X, y, 5)\n",
      "    print 'acc=', acc\n",
      "    return acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 490
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_all(test_tweet, use_descr=True, use_text=True)\n",
      "run_all(test_tweet, use_descr=True, use_text=False)\n",
      "run_all(test_tweet, use_descr=False, use_text=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "113036 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.745353158087\n",
        "84087 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.75453537955\n",
        "41323 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.625624151106\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 491,
       "text": [
        "0.62562415110615821"
       ]
      }
     ],
     "prompt_number": 491
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}