{
 "metadata": {
  "name": "",
  "signature": "sha256:a910a5e4ec2292a14228abe9da7f6b554b0aef7aade4aab344208f6a79ac7780"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.Collect the data have \"#TheWalkingDead\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ConfigParser\n",
      "from TwitterAPI import TwitterAPI,TwitterRestPager\n",
      "import sys\n",
      "import time\n",
      "import ast,re\n",
      "\n",
      "def get_twitter(config_file):\n",
      "    config = ConfigParser.ConfigParser()\n",
      "    config.read(config_file)\n",
      "    twitter = TwitterAPI(\n",
      "                   config.get('twitter', 'consumer_key'),\n",
      "                   config.get('twitter', 'consumer_secret'),\n",
      "                   config.get('twitter', 'access_token'),\n",
      "                   config.get('twitter', 'access_token_secret'))\n",
      "    return twitter\n",
      "twitter = get_twitter('twitter1.cfg')\n",
      "print 'Established Twitter connection.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Established Twitter connection.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data(string,filename):\n",
      "\tpager = TwitterRestPager(twitter,'search/tweets',{'q':'#TheWalkingDead','count':200})\n",
      "\ti=0\n",
      "\tf=open(filename,'w')\n",
      "\tfor item in pager.get_iterator(new_tweets=True):\n",
      "\t\tf.write(str(item)+'\\n')\n",
      "\t\ti=i+1\n",
      "\t\tprint i\n",
      "\tf.close\n",
      "\n",
      "\n",
      "\n",
      "# get_data('#TheWalkingDead','TheWalkingDead2.txt')\n",
      "print 'ok'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_date(filename):\n",
      "\tcount=0\n",
      "\tf=open(filename,'r')\n",
      "\tfor i in f:\n",
      "\t\ti=ast.literal_eval(i)\n",
      "\t\tif i.has_key('created_at'):\n",
      "\t\t\tcount=count+1\n",
      "\t\t\tprint i['created_at']\n",
      "\t\t\tprint count\n",
      "\t\telse:\n",
      "\t\t\ti.clear()\n",
      "# get_date('TheWalkingDead.txt')\n",
      "# get_date('TheWalkingDead20.txt')\n",
      "# get_date('TheWalkingDead26.txt')\n",
      "# total have 414,700 data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Get data from txt file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# return a list of tuple that have user name, user text and user description\n",
      "import ast\n",
      "def get_des(filename):\n",
      "    f=open(filename,'r')\n",
      "    info=[]\n",
      "    for i in f:\n",
      "        i=ast.literal_eval(i)\n",
      "        if i['lang']==u'en':\n",
      "            info.append((i['user']['name'].encode('utf-8'),i['text'].encode('utf-8'),i['user']['description'].encode('utf-8')))\n",
      "    f.close()\n",
      "    return info\n",
      "# a=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead20.txt')\n",
      "# b=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead21-25.txt')\n",
      "# c=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead26.txt')\n",
      "# d=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead27-03.txt')\n",
      "info_pos=get_des('/Users/zhaoyixuan/Documents/cs579/positive.txt')\n",
      "b=get_des('/Users/zhaoyixuan/Documents/cs579/negative.txt')\n",
      "c=get_des('/Users/zhaoyixuan/Documents/cs579/negativefuck.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(info_pos),len(b),len(c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2905 394 1364\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add all the data into info list\n",
      "info_neg=[]\n",
      "count=0\n",
      "g=[b,c]\n",
      "for i in g:\n",
      "    for x in i:\n",
      "        info_neg.append(x)\n",
      "        count+=1\n",
      "print count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1758\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of corresponding name, text and description\n",
      "name_test=[]\n",
      "text_test=[]\n",
      "description_test=[]\n",
      "for i in info:\n",
      "    name_test.append(i[0])\n",
      "    text_test.append(i[1])\n",
      "    description.append(i[2])\n",
      "print 'Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(info, open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'wb'))\n",
      "pickle.dump(name, open('/Users/zhaoyixuan/Documents/cs579/name_test.pkl', 'wb'))\n",
      "pickle.dump(text, open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'wb'))\n",
      "pickle.dump(description, open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'wb'))\n",
      "pickle.dump(info1,open('/Users/zhaoyixuan/Documents/cs579/info1.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "info=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'rb'))\n",
      "name=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/name.pkl', 'rb'))\n",
      "text=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'rb'))\n",
      "description=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'rb'))\n",
      "info1=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/info1.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print info_pos[:10]\n",
      "print info_neg[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('The Walking Dead', \"RT @IGN: Walking Dead producer delves into Rick &amp; Daryl's new dynamic. Plus, why'd Sasha do THAT? http://t.co/gzVcmnt3aY http://t.co/yIjjYF\\xe2\\x80\\xa6\", 'Sundays @ 9|8c. Only on @AMC_TV.'), ('The Walking Dead', 'Walking Dead Producer Gale Anne Hurd on Rick Becoming the New Shane  http://t.co/piVn6OL2tC @GunnerGale', 'Sundays @ 9|8c. Only on @AMC_TV.'), ('The Walking Dead', 'The Walking Dead jumps across storylines in midseason finale -  http://t.co/MIE1QNKjCR', 'Sundays @ 9|8c. Only on @AMC_TV.'), ('The Walking Dead', 'RT @TWDForums: Norman Reedus says Daryl will do \"anything\" to get back the women he cares about in the mid-season finale.\\n\\nhttp://t.co/ZswI\\xe2\\x80\\xa6', 'Sundays @ 9|8c. Only on @AMC_TV.'), ('The Walking Dead', \"RT @Popdust: .@WalkingDead_AMC's mid-season finale is shaping up to be pretty intense! http://t.co/GLGC2076p8 @TheWalkingFans http://t.co/C\\xe2\\x80\\xa6\", 'Sundays @ 9|8c. Only on @AMC_TV.'), ('The Walking Dead', '@Amore_kc no way!', 'Sundays @ 9|8c. Only on @AMC_TV.'), ('The Walking Dead', 'RT @AspectRatio235: Another solid @TheWalkingDead episode tonight! They should do the skull-ripping gag more often. @TheWalkingFans', 'Sundays @ 9|8c. Only on @AMC_TV.'), ('The Walking Dead', 'RT @Walker_Stalkers: Who dies in this mid-season finale of #TheWalkingDead?', 'Sundays @ 9|8c. Only on @AMC_TV.'), ('The Walking Dead', \"'The Walking Dead' Season 5 spoilers: Episode 7 'Crossed'  http://t.co/YhiAmu4Ycm\", 'Sundays @ 9|8c. Only on @AMC_TV.'), ('The Walking Dead', 'Great show tonight just one episode left! #MIDSEASONFINALE  \\xf0\\x9f\\x91\\xa3\\xf0\\x9f\\x92\\x80', 'Sundays @ 9|8c. Only on @AMC_TV.')]\n",
        "[('\\xe2\\xad\\x90Starla\\xe2\\xad\\x90', \"I can't believe there is only 1 more new The Walking Dead episode this year. #sucks\", 'Loves:  Music, travel, The Walking Dead, Game of Thrones.'), ('CJ', 'the walking dead sucks', 'Follower of Christ | Atmore | Jer. 29:11'), ('Refrigerator Dweller', \"@nPwn That sucks, I'm sorry.  I'm up because I decided that watching the walking dead would be nice\", \"I don't know what I'm doing.\"), ('Apple Bratt', 'Walking dead marathon tonight. Sucks that Ill be home alone all scared \\xf0\\x9f\\x98\\xb0.', \"Food\\xe2\\x80\\xa2 Fitness\\xe2\\x80\\xa2 Travel\\xe2\\x80\\xa2 Fashion\\xe2\\x80\\xa2 Snowboarding\\xe2\\x80\\xa2 Dance\\xe2\\x80\\xa2 [I'm adventurous]\"), ('Aimee\\xe2\\x99\\xa5Lee', '@Serggf07 @SonsofAnarchy @sutterink lol well it sucks for you! I can catch up! And on walking dead too\\xf0\\x9f\\x99\\x8c', \"Through every dark night; there's a brighter day...\\xe2\\x9d\\xa4\"), ('Zombie Times', '@funnyordie @FredTheWalker bulkshit sell out. Walking Dead sucks. Here is why http://t.co/P0vkytcPn5', 'Dead and alive'), ('Julez Mirchoff', 'RT @JenileeXO: This girl just said The Walking Dead sucks. #ByeFelicia', 'Psalm 16:8'), ('Larry the Lobster', 'This girl just said The Walking Dead sucks. #ByeFelicia', '18.'), ('Jack Halliday', '@JesusCortes978 @Monstermartins walking dead sucks. Like you. Dexter is good', 'Mr. Caouette calls me clifford, Dr. Hart calls me the spawn of satan'), ('Victor FBGM \\xe2\\x9c\\x88', '\"@JavierMonci: @V_Lopez8 breaking bad is not interesting at all\"walking dead sucks dick', '#sTRONg')]\n"
       ]
      }
     ],
     "prompt_number": 148
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.Filter data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fetch male/female names from Census.\n",
      "import requests\n",
      "\n",
      "def get_census_names():\n",
      "    \"\"\" Fetch a list of common male/female names from the census.\n",
      "    For ambiguous names, we select the more frequent gender.\"\"\"\n",
      "    males = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first').text.split('\\n')\n",
      "    females = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first').text.split('\\n')\n",
      "    males_pct = dict([(m.split()[0].lower(), float(m.split()[1]))\n",
      "                  for m in males if m])\n",
      "    females_pct = dict([(f.split()[0].lower(), float(f.split()[1]))\n",
      "                    for f in females if f])\n",
      "    male_names = set([m for m in males_pct if m not in females_pct or\n",
      "                  males_pct[m] > females_pct[m]])\n",
      "    female_names = set([f for f in females_pct if f not in males_pct or\n",
      "                  females_pct[f] > males_pct[f]])    \n",
      "    return male_names, female_names\n",
      "\n",
      "male_names, female_names = get_census_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "from collections import Counter\n",
      "def get_first_name(t):\n",
      "    parts =t.split()\n",
      "    if len(parts)>0:\n",
      "        return parts[0].lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter the data and keep the first name in male_names of female_names\n",
      "def sample(name_test, male_names, female_names):\n",
      "    count=0\n",
      "    real_name=[]\n",
      "    num=[]\n",
      "#     for t in name_test:\n",
      "#         name = get_first_name(t)\n",
      "#         if name in male_names or name in female_names:\n",
      "#             real_name.append(name)\n",
      "#             num.append(i)\n",
      "#         i+=1\n",
      "#     i=0\n",
      "    for t in name_test:\n",
      "        a=re.sub('\\W+',' ',t).lower().split()\n",
      "        if len(a)>1:\n",
      "            for i in range(0,len(a)-1):\n",
      "                if a[i] in male_names :\n",
      "                    real_name.append(t)\n",
      "                    num.append(count)\n",
      "                if a[i] in female_names:\n",
      "                    real_name.append(t)\n",
      "                    num.append(count)\n",
      "        elif len(a)==1:\n",
      "            if a[0] in male_names:\n",
      "                real_name.append(t)\n",
      "                num.append(count)\n",
      "            if a[0] in female_names:\n",
      "                real_name.append(t)\n",
      "                num.append(count)\n",
      "        count+=1\n",
      "    return real_name,num"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "real_name,num=sample(name,male_names,female_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(num)\n",
      "print name[:30]\n",
      "print len(real_name)\n",
      "print num[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "370967\n",
        "['YayasHere', 'Shar-monster', 'The Running Dead.', 'AbrahamFordTWD', \"Pete's Basement\", 'Chad', 'Sammy Cannons', 'katie', 'sugar tits ', 'Aubertus Ivan', 'RedPoison ', 'Sky\\xe2\\x99\\xa1', 'MattGarratt\\xe2\\x84\\xa2', 'Abraham Ford ', 'Locious Lawliet', 'Fernando Caviedes \\xef\\xa3\\xbf', 'tammie parker', 'katherine daley ', 'angela villasanta', 'Raven', 'YayasHere', 'Castle M\\xc3\\xa9xico.', 'Ally Waldron', 'Q u e e n  D o n n a', 'Phatt Matt', 'sn', 'holly wilson', 'Sam Lawson', 'Kailey Kirk\\xe2\\x9c\\x8c\\xef\\xb8\\x8f', 'tammie parker']\n",
        "370967\n",
        "[4, 5, 6, 7, 13, 15, 16, 17, 18, 19, 26, 27, 29, 30, 31, 34, 36, 37, 42, 43]\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# info1 store the filtered data\n",
      "info1=[]\n",
      "for i in num:\n",
      "    info1.append(info[i])\n",
      "print len(info1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "370967\n"
       ]
      }
     ],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print info1[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(\"Pete's Basement\", 'RT @BobBobsleg: I taste like chicken #TheWalkingDead #BobBQ', \"The #comicbook talk show from real New Yorkers!  From #NCBD to the #SilverAge. Join the crew every week in Pete's Basement. Trailer: http://t.co/c1qF11vPzY\"), ('Chad', \"Coach Muschamp is the patient zero of #TheWalkingDead cause he's a zombie\", ''), ('Sammy Cannons', 'bloody hell that ending was a bit grim #TheWalkingDead', '18, Bromsgrove'), ('katie', 'RT @n1ghtlock: THIS IS TOO MUCH #TheWalkingDeadUK #TheWalkingDead http://t.co/xZ9JAarXpw', '\\xe2\\x99\\xa1 ariana, youtubers, twd, pll, ahs.. basically mixed fan account \\xe2\\x99\\xa1'), ('Abraham Ford ', '@BAMFAbraham ^^^^^^^^^^ I get the bus running and look what happens. #smh #TheWalkingDead', 'The Thing About Smart Motherfuckers Is They Sound Like Crazy Motherfuckers To Dumb Motherfuckers #ThewalkingDead #TWDFamily #AbrahamsArmy'), ('Fernando Caviedes \\xef\\xa3\\xbf', \"I'm watching #TheWalkingDead with 94 others on #tvtag http://t.co/k1y4IE4AUD\", 'Si toda cosa ocurriera como la hubiera planeado nunca experimentar\\xc3\\xada algo nuevo. Mi vida ser\\xc3\\xada una repetici\\xc3\\xb3n infinita de viejos resultados.'), ('tammie parker', 'RT @TheWalkingNews: Best.Selfie.Ever. #TheWalkingDead http://t.co/7FhZhwBLzC', 'with great power comes great responsiblity'), ('katherine daley ', \"RT @WalkingDead_AMC: Stay off the roads. Stay in the forest and go inside last night's #TheWalkingDead. http://t.co/7w3xnTFWpQ\", \"Mom of 2 Teenagers! Born&raised n Dayton Ohio! I'm addicted to the walking dead!\"), ('angela villasanta', 'watched #TheWalkingDead s05e02 before breakfast/going to work \\xf0\\x9f\\x98\\x8a\\xf0\\x9f\\x91\\xa4\\xf0\\x9f\\x91\\xa4\\xf0\\x9f\\x91\\xa3\\xf0\\x9f\\x91\\xa3', \"a working mom, loving mother, wife, sister, friend and daughter :D kinda OC and loving #pinterest so much! \\xe2\\x99\\xa5 plus i'm a witch \\xe2\\x98\\x86^-^\"), ('Raven', \"RT @WalkingDead_AMC: Stay off the roads. Stay in the forest and go inside last night's #TheWalkingDead. http://t.co/7w3xnTFWpQ\", 'Walking Dead Addict, @JoshMcDermitt Sept22 @theoduscrane/May22 (Tiny)  @OfficialKesan/Aug11 @zwgman also follows :) Nature, music Boondock Saints')]\n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Sentiment analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from StringIO import StringIO\n",
      "from zipfile import ZipFile\n",
      "from urllib import urlopen\n",
      "\n",
      "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
      "zipfile = ZipFile(StringIO(url.read()))\n",
      "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
      "\n",
      "afinn = dict()\n",
      "\n",
      "for line in afinn_file:\n",
      "    parts = line.strip().split()\n",
      "    if len(parts) == 2:\n",
      "        afinn[parts[0]] = int(parts[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'read', len(afinn), 'AFINN terms.\\nE.g.:', afinn.items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 2462 AFINN terms.\n",
        "E.g.: [('limited', -1), ('suicidal', -2), ('pardon', 2), ('desirable', 2), ('protest', -2)]\n"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def afinn_sentiment(terms, afinn):\n",
      "    total = 0.\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            total += afinn[t]\n",
      "    return total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def afinn_sentiment2(terms, afinn, verbose=False):\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            if verbose:\n",
      "                print '\\t%s=%d' % (t, afinn[t])\n",
      "            if afinn[t] > 0:\n",
      "                pos += afinn[t]\n",
      "            else:\n",
      "                neg += 1 * afinn[t]\n",
      "    return pos, neg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score=[]\n",
      "for tweet in info_pos:\n",
      "    pos,neg= afinn_sentiment2(tweet[1].split(), afinn)\n",
      "    if pos>-1*neg:\n",
      "        score.append((''.join(tweet[1]),4))\n",
      "    elif pos<-1*neg:\n",
      "        score.append((''.join(tweet[1]),2))\n",
      "    elif pos==-1*neg:\n",
      "        score.append((''.join(tweet[1]),0))\n",
      "score_neg=[]\n",
      "for tweet in info_neg:\n",
      "    pos,neg= afinn_sentiment2(tweet[1].split(), afinn)\n",
      "    if pos>-1*neg:\n",
      "        score.append((''.join(tweet[1]),4))\n",
      "    elif pos<-1*neg:\n",
      "        score.append((''.join(tweet[1]),2))\n",
      "    elif pos==-1*neg:\n",
      "        score.append((''.join(tweet[1]),0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print score[0][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n"
       ]
      }
     ],
     "prompt_number": 163
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# positive=4,negative=2,neutral=0\n",
      "total=[]\n",
      "for tweet in info1:\n",
      "    pos,neg= afinn_sentiment2(tweet[1].split(), afinn)\n",
      "    if pos>-1*neg:\n",
      "        total.append((''.join(tweet[1]),4))\n",
      "    elif pos<-1*neg:\n",
      "        total.append((''.join(tweet[1]),2))\n",
      "    elif pos==-1*neg:\n",
      "        total.append((''.join(tweet[1]),0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total1=[]\n",
      "for tweet in info1:\n",
      "    tweet=tweet[1].split()\n",
      "    t=afinn_sentiment(tweet,afinn)\n",
      "    total1.append((' '.join(tweet),t))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "import numpy as np\n",
      "# y = np.array([t[1] for t in total])\n",
      "# y1 = np.array([t[1] for t in total1])\n",
      "y2=np.array([t[1] for t in score])\n",
      "# print 'label counts=', Counter(y)\n",
      "# print 'label counts=', Counter(y1)\n",
      "print len(y2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4663\n"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer()\n",
      "# X = vectorizer.fit_transform(t[1] for t in info1)\n",
      "X = vectorizer.fit_transform(t[0] for t in score)\n",
      "print 'vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vectorized 4663 tweets. found 7770 terms.\n"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(X, y)\n",
      "model = LogisticRegression()\n",
      "model.fit(X, y1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print 'avg accuracy', do_cross_val(X, y, 5)\n",
      "# print 'avg accuracy', do_cross_val(X, y1, 5)\n",
      "print 'avg accuracy', do_cross_val(X,y2, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.707233806678\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.Gender prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(string, lowercase, keep_punctuation, prefix,\n",
      "             collapse_urls, collapse_mentions):\n",
      "    if not string:\n",
      "        return []\n",
      "    if lowercase:\n",
      "        string = string.lower()\n",
      "    tokens = []\n",
      "    if collapse_urls:\n",
      "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
      "    if collapse_mentions:\n",
      "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
      "    if keep_punctuation:\n",
      "        tokens = string.split()\n",
      "    else:\n",
      "        tokens = re.sub('\\W+', ' ', string).split()\n",
      "    if prefix:\n",
      "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
      "    return tokens\n",
      "\n",
      "def tweet2tokens(infomation, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True):\n",
      "    tokens = tokenize(infomation[1], lowercase, keep_punctuation, None,\n",
      "                       collapse_urls, collapse_mentions)\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(infomation[2], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(info1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "330296\n"
       ]
      }
     ],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's tokenize all tweets.\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=False, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True)\n",
      "              for t in info1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "330296\n"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Store these in a sparse matrix.\n",
      "from collections import defaultdict\n",
      "\n",
      "def make_vocabulary(tokens_list):\n",
      "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
      "    for tokens in tokens_list:\n",
      "        for token in tokens:\n",
      "            vocabulary[token]\n",
      "    print '%d unique terms in vocabulary' % len(vocabulary)\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "111835 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert features to a sparse matrix X.\n",
      "# X[i,j] is the frequency of term j in tweet i\n",
      "# \n",
      "from scipy.sparse import lil_matrix\n",
      "\n",
      "def make_feature_matrix(tokens_list, vocabulary):\n",
      "    X = lil_matrix((len(info1), len(vocabulary)))\n",
      "    for i, tokens in enumerate(tokens_list):\n",
      "        for token in tokens:\n",
      "            j = vocabulary[token]\n",
      "            X[i,j] += 1\n",
      "    return X.tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = make_feature_matrix(tokens_list, vocabulary)\n",
      "print 'shape of X:', X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shape of X: (330296, 111835)\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute z = X * \\beta, where X is a CSR matrix.\n",
      "import numpy as np\n",
      "beta = np.ones(len(vocabulary))  # assume Beta = vector of 1s\n",
      "z = np.zeros(len(info1))\n",
      "for i in range(len(info1)):  # for each row.\n",
      "    for j in range(X.indptr[i], X.indptr[i+1]): # for each col.\n",
      "        colidx = X.indices[j]\n",
      "        z[i] += beta[colidx] * X.data[j]\n",
      "print z[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 13.  11.  18.  29.  34.]\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# y is a 1d numpy array of gender labels.\n",
      "# Let 1=Female, 0=Male.\n",
      "import numpy as np\n",
      "\n",
      "def get_gender(t, male_names, female_names):\n",
      "    name_label = get_first_name(t)\n",
      "    if name_label in female_names:\n",
      "        return 1\n",
      "    elif name_label in male_names:\n",
      "        return 0\n",
      "    else:\n",
      "        return -1\n",
      "    \n",
      "y = np.array([get_gender(t, male_names, female_names) for t in real_name])\n",
      "print 'gender labels:', Counter(y).items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gender labels: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 131151), (1, 199145)]\n"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Do 5-fold cross-validation\n",
      "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val(X, y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "acc= 0.744716924008\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.810881181974\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.793154604217\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.785661302775\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.815589094597\n",
        "0.790000621514\n"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tweet2tokens(info, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    tokens = []\n",
      "    if use_text:\n",
      "        tokens.extend(tokenize(info[1], lowercase, keep_punctuation, None,\n",
      "                           collapse_urls, collapse_mentions))\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(info[2], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_all(info, use_descr=True, lowercase=True,\n",
      "            keep_punctuation=True, descr_prefix=None,\n",
      "            collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    \n",
      "    tokens_list = [tweet2tokens(t, use_descr, lowercase,\n",
      "                            keep_punctuation, descr_prefix,\n",
      "                            collapse_urls, collapse_mentions, use_text)\n",
      "                  for t in info1]\n",
      "    vocabulary = make_vocabulary(tokens_list)\n",
      "    X = make_feature_matrix(tokens_list, vocabulary)\n",
      "    acc = do_cross_val(X, y, 5)\n",
      "    print 'acc=', acc\n",
      "    return acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_all(info1, use_descr=True, use_text=True)\n",
      "run_all(info1, use_descr=True, use_text=False)\n",
      "run_all(info1, use_descr=False, use_text=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "215094 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.770829548895\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.82235577287\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.801934634191\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.789566902315\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.832286289529\n",
        "acc= 0.80339462956\n",
        "146124 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.79485316379\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.843382430857\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.822673670507\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.807202652175\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.843972812183\n",
        "acc= 0.822416945902\n",
        "91978 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.609960641841\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.587641350914\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597148004057\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.585658275178\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.589230839098\n",
        "acc= 0.593927822218\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 153,
       "text": [
        "0.59392782221758778"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import product\n",
      "use_descr_opts = [True, False]\n",
      "lowercase_opts = [True, False]\n",
      "keep_punctuation_opts = [True, False]\n",
      "descr_prefix_opts = ['d=', '']\n",
      "url_opts = [True, False]\n",
      "mention_opts = [True, False]\n",
      "use_text_opts = [True, False]\n",
      "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention', 'use_text']\n",
      "option_iter = product(use_descr_opts, lowercase_opts,\n",
      "                       keep_punctuation_opts,\n",
      "                       descr_prefix_opts, url_opts,\n",
      "                       mention_opts, use_text_opts)\n",
      "results = []\n",
      "for options in option_iter:\n",
      "    if options[0] or options[-1]:  # skip options that don't use descr or text.\n",
      "        acc = run_all(info1, *options)\n",
      "        results.append((options, acc))\n",
      "best_result = max(results, key=lambda x: x[1])\n",
      "print 'best result:\\n\\tacc=%g\\n\\toptions=%s' % (best_result[1],\n",
      "                                                zip(argnames, best_result[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "238102 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.811239116894\n",
        "146124 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.822416945902\n",
        "268839 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.817003617721\n",
        "155361 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.824179002782\n",
        "265718 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.812407763201\n",
        "149285 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.823185953499\n",
        "296456 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.81754253206\n",
        "158522 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.824863238592\n",
        "215094 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.80339462956\n",
        "146124 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.822416945902\n",
        "245515 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.809077387297\n",
        "155361 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.824179002782\n",
        "242713 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.803885099787\n",
        "149285 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.823185953499\n",
        "273136 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.809555738568\n",
        "158522 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.824863238592\n",
        "111835 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.790000621514\n",
        "72594 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.799946181195\n",
        "138593 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.800703081568\n",
        "80947 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.804439123831\n",
        "139416 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.791759649972\n",
        "75799 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.800860513013\n",
        "166219 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.80041245329\n",
        "84155 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.805371620999\n",
        "96053 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.778949901971\n",
        "72594 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.799946181195\n",
        "122055 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.790394134544\n",
        "80947 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.804439123831\n",
        "123604 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.778389814531\n",
        "75799 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.800860513013\n",
        "149648 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.789749258863\n",
        "84155 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.805371620999\n",
        "283340 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.81605903024\n",
        "172422 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.827799988769\n",
        "314398 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.819967622133\n",
        "181862 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.829359195763\n",
        "310967 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.817291256763\n",
        "175583 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.828411562308\n",
        "342026 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.820415714579\n",
        "185023 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.829910217692\n",
        "256538 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.808947235294\n",
        "172422 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.827799988769\n",
        "287317 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.813309961947\n",
        "181862 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.829359195763\n",
        "284168 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.809631460609\n",
        "175583 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.828411562308\n",
        "314949 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.812816473931\n",
        "185023 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.829910217692\n",
        "146533 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.799973470124\n",
        "94230 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.810479152122\n",
        "173837 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.806921759779\n",
        "102960 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.813976017925\n",
        "174134 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.801762768085\n",
        "97436 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8112935733\n",
        "201483 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.806691670324\n",
        "106169 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.815099250101\n",
        "126173 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.789734155071\n",
        "94230 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.810479152122\n",
        "152855 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.797726960674\n",
        "102960 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.813976017925\n",
        "153759 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.789843153908\n",
        "97436 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8112935733\n",
        "180484 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.796948881058\n",
        "106169 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.815099250101\n",
        "91978 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.593927822218\n",
        "113478 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.604354742295\n",
        "116433 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-157-a4d95fd73e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moption_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# skip options that don't use descr or text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mbest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-151-05793fab23f3>\u001b[0m in \u001b[0;36mrun_all\u001b[0;34m(info, use_descr, lowercase, keep_punctuation, descr_prefix, collapse_urls, collapse_mentions, use_text)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_feature_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_cross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'acc='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-154-bcdb4309e9a3>\u001b[0m in \u001b[0;36mdo_cross_val\u001b[0;34m(X, y, nfolds)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/zhaoyixuan/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    701\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                                               rnd.randint(np.iinfo('i').max))\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}