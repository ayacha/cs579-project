{
 "metadata": {
  "name": "",
  "signature": "sha256:e8699260ac6e475f4ea66ac413c837949e0621a4b6c6d6478ab231c3e0929b2c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.Collect the data have \"#TheWalkingDead\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ConfigParser\n",
      "from TwitterAPI import TwitterAPI,TwitterRestPager\n",
      "import sys\n",
      "import time\n",
      "import ast,re\n",
      "\n",
      "def get_twitter(config_file):\n",
      "    config = ConfigParser.ConfigParser()\n",
      "    config.read(config_file)\n",
      "    twitter = TwitterAPI(\n",
      "                   config.get('twitter', 'consumer_key'),\n",
      "                   config.get('twitter', 'consumer_secret'),\n",
      "                   config.get('twitter', 'access_token'),\n",
      "                   config.get('twitter', 'access_token_secret'))\n",
      "    return twitter\n",
      "twitter = get_twitter('twitter1.cfg')\n",
      "print 'Established Twitter connection.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Established Twitter connection.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data(string,filename):\n",
      "\tpager = TwitterRestPager(twitter,'search/tweets',{'q':'#TheWalkingDead','count':200})\n",
      "\ti=0\n",
      "\tf=open(filename,'w')\n",
      "\tfor item in pager.get_iterator(new_tweets=True):\n",
      "\t\tf.write(str(item)+'\\n')\n",
      "\t\ti=i+1\n",
      "\t\tprint i\n",
      "\tf.close\n",
      "\n",
      "\n",
      "\n",
      "# get_data('#TheWalkingDead','TheWalkingDead2.txt')\n",
      "print 'ok'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_date(filename):\n",
      "\tcount=0\n",
      "\tf=open(filename,'r')\n",
      "\tfor i in f:\n",
      "\t\ti=ast.literal_eval(i)\n",
      "\t\tif i.has_key('created_at'):\n",
      "\t\t\tcount=count+1\n",
      "\t\t\tprint i['created_at']\n",
      "\t\t\tprint count\n",
      "\t\telse:\n",
      "\t\t\ti.clear()\n",
      "# get_date('TheWalkingDead.txt')\n",
      "# get_date('TheWalkingDead20.txt')\n",
      "# get_date('TheWalkingDead26.txt')\n",
      "# total have 414,700 data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Get data from txt file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# return a list of tuple that have user name, user text and user description\n",
      "import ast\n",
      "def get_des(filename):\n",
      "    f=open(filename,'r')\n",
      "    info=[]\n",
      "    for i in f:\n",
      "        i=ast.literal_eval(i)\n",
      "        if i['lang']==u'en':\n",
      "            info.append((i['user']['name'].encode('utf-8'),i['text'].encode('utf-8'),i['user']['description'].encode('utf-8')))\n",
      "    f.close()\n",
      "    return info\n",
      "# a=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead20.txt')\n",
      "# b=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead21-25.txt')\n",
      "# c=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead26.txt')\n",
      "# d=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead27-03.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(a),len(b),len(c),len(d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "79514 57728 19491 535787\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add all the data into info list\n",
      "info=[]\n",
      "count=0\n",
      "g=[a,b,c,d]\n",
      "for i in g:\n",
      "    for x in i:\n",
      "        info.append(x)\n",
      "        count+=1\n",
      "print count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "692520\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of corresponding name, text and description\n",
      "name=[]\n",
      "text=[]\n",
      "description=[]\n",
      "for i in info:\n",
      "    name.append(i[0])\n",
      "    text.append(i[1])\n",
      "    description.append(i[2])\n",
      "print 'Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(info, open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'wb'))\n",
      "pickle.dump(name, open('/Users/zhaoyixuan/Documents/cs579/name.pkl', 'wb'))\n",
      "pickle.dump(text, open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'wb'))\n",
      "pickle.dump(description, open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'wb'))\n",
      "pickle.dump(info1,open('/Users/zhaoyixuan/Documents/cs579/info1.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "# info=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'rb'))\n",
      "name=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/name.pkl', 'rb'))\n",
      "text=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'rb'))\n",
      "description=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'rb'))\n",
      "info1=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/info1.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(info1),len(name),len(text),len(description)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "330296 692520 692520 692520\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.Filter data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fetch male/female names from Census.\n",
      "import requests\n",
      "\n",
      "def get_census_names():\n",
      "    \"\"\" Fetch a list of common male/female names from the census.\n",
      "    For ambiguous names, we select the more frequent gender.\"\"\"\n",
      "    males = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first').text.split('\\n')\n",
      "    females = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first').text.split('\\n')\n",
      "    males_pct = dict([(m.split()[0].lower(), float(m.split()[1]))\n",
      "                  for m in males if m])\n",
      "    females_pct = dict([(f.split()[0].lower(), float(f.split()[1]))\n",
      "                    for f in females if f])\n",
      "    male_names = set([m for m in males_pct if m not in females_pct or\n",
      "                  males_pct[m] > females_pct[m]])\n",
      "    female_names = set([f for f in females_pct if f not in males_pct or\n",
      "                  females_pct[f] > males_pct[f]])    \n",
      "    return male_names, female_names\n",
      "\n",
      "male_names, female_names = get_census_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "from collections import Counter\n",
      "def get_first_name(t):\n",
      "    parts =t.split()\n",
      "    if len(parts)>0:\n",
      "        return parts[0].lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter the data and keep the first name in male_names of female_names\n",
      "def sample(name, male_names, female_names):\n",
      "    i=0\n",
      "    real_name=[]\n",
      "    num=[]\n",
      "    for t in name:\n",
      "        name = get_first_name(t)\n",
      "        if name in male_names or name in female_names:\n",
      "            real_name.append(name)\n",
      "            num.append(i)\n",
      "        i+=1\n",
      "    i=0\n",
      "    for t in name:\n",
      "        a=re.sub('\\W+',' ',t).lower().split()\n",
      "        for i in range(0,len(a)-1):\n",
      "            if a[i] in male_names or a[i] in female_names:\n",
      "                real_name.append(t)\n",
      "                num.append(i)\n",
      "    i+=1\n",
      "    return real_name,num"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "real_name,num=sample(name,male_names,female_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# info1 store the filtered data\n",
      "info1=[]\n",
      "for i in num:\n",
      "    info1.append(info[i])\n",
      "print len(info1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'info' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-85cc11030f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minfo1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minfo1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'info' is not defined"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Sentiment analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from StringIO import StringIO\n",
      "from zipfile import ZipFile\n",
      "from urllib import urlopen\n",
      "\n",
      "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
      "zipfile = ZipFile(StringIO(url.read()))\n",
      "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
      "\n",
      "afinn = dict()\n",
      "\n",
      "for line in afinn_file:\n",
      "    parts = line.strip().split()\n",
      "    if len(parts) == 2:\n",
      "        afinn[parts[0]] = int(parts[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'read', len(afinn), 'AFINN terms.\\nE.g.:', afinn.items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 2462 AFINN terms.\n",
        "E.g.: [('limited', -1), ('suicidal', -2), ('pardon', 2), ('desirable', 2), ('protest', -2)]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def afinn_sentiment(terms, afinn):\n",
      "    total = 0.\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            total += afinn[t]\n",
      "    return total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def afinn_sentiment2(terms, afinn):\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for t in terms.split():\n",
      "        if t in afinn:\n",
      "            if afinn[t] > 0:\n",
      "                pos += afinn[t]\n",
      "            else:\n",
      "                neg += afinn[t]\n",
      "    return pos, neg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# positive=4,negatibe=2,neutral=0\n",
      "total=[]\n",
      "for tweet in info1:\n",
      "    pos, neg = afinn_sentiment2(tweet[1], afinn)\n",
      "    if pos > -1*neg:\n",
      "        total.append((''.join(tweet[1]),4))\n",
      "    elif -1*neg > pos:\n",
      "        total.append((''.join(tweet[1]),2))\n",
      "    elif -1*neg==pos:\n",
      "        total.append((''.join(tweet[1]),0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total1=[]\n",
      "for tweet in info1:\n",
      "    tweet=tweet[1].split()\n",
      "    t=afinn_sentiment(tweet,afinn)\n",
      "    total1.append((' '.join(tweet),t))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "import numpy as np\n",
      "y = np.array([t[1] for t in total])\n",
      "y1 = np.array([t[1] for t in total1])\n",
      "print 'label counts=', Counter(y)\n",
      "print 'label counts=', Counter(y1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "label counts= "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({0: 216763, 4: 62836, 2: 50697})\n",
        "label counts= "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({0.0: 216763, 2.0: 18572, 3.0: 14373, -3.0: 13557, -2.0: 13309, 4.0: 10218, 1.0: 9182, -1.0: 9033, 6.0: 8754, -4.0: 8637, -5.0: 1968, -6.0: 1752, -7.0: 1276, 5.0: 1231, -8.0: 547, -9.0: 232, 7.0: 215, -10.0: 203, 8.0: 136, 9.0: 69, -11.0: 59, -12.0: 58, 10.0: 52, -14.0: 22, -13.0: 22, 11.0: 20, 12.0: 7, -16.0: 7, -17.0: 5, 13.0: 4, -15.0: 3, -21.0: 2, -18.0: 2, 14.0: 1, 15.0: 1, 27.0: 1, -32.0: 1, -22.0: 1, -19.0: 1})\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(t[1] for t in info1)\n",
      "print 'vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vectorized 330296 tweets. found 82044 terms.\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(X, y)\n",
      "model = LogisticRegression()\n",
      "model.fit(X, y1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "clf = SVC()\n",
      "clf.fit(X, y) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val(X, y, 5)\n",
      "print 'avg accuracy', do_cross_val(X, y1, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.844530506007\n",
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.77879549079\n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.Gender prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(string, lowercase, keep_punctuation, prefix,\n",
      "             collapse_urls, collapse_mentions):\n",
      "    if not string:\n",
      "        return []\n",
      "    if lowercase:\n",
      "        string = string.lower()\n",
      "    tokens = []\n",
      "    if collapse_urls:\n",
      "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
      "    if collapse_mentions:\n",
      "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
      "    if keep_punctuation:\n",
      "        tokens = string.split()\n",
      "    else:\n",
      "        tokens = re.sub('\\W+', ' ', string).split()\n",
      "    if prefix:\n",
      "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
      "    return tokens\n",
      "\n",
      "def tweet2tokens(infomation, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True):\n",
      "    tokens = tokenize(infomation[1], lowercase, keep_punctuation, None,\n",
      "                       collapse_urls, collapse_mentions)\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(infomation[2], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(info1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "330296\n"
       ]
      }
     ],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's tokenize all tweets.\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=False, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True)\n",
      "              for t in info1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "330296\n"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Store these in a sparse matrix.\n",
      "from collections import defaultdict\n",
      "\n",
      "def make_vocabulary(tokens_list):\n",
      "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
      "    for tokens in tokens_list:\n",
      "        for token in tokens:\n",
      "            vocabulary[token]\n",
      "    print '%d unique terms in vocabulary' % len(vocabulary)\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "111835 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert features to a sparse matrix X.\n",
      "# X[i,j] is the frequency of term j in tweet i\n",
      "# \n",
      "from scipy.sparse import lil_matrix\n",
      "\n",
      "def make_feature_matrix(tokens_list, vocabulary):\n",
      "    X = lil_matrix((len(info1), len(vocabulary)))\n",
      "    for i, tokens in enumerate(tokens_list):\n",
      "        for token in tokens:\n",
      "            j = vocabulary[token]\n",
      "            X[i,j] += 1\n",
      "    return X.tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = make_feature_matrix(tokens_list, vocabulary)\n",
      "print 'shape of X:', X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shape of X: (330296, 111835)\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute z = X * \\beta, where X is a CSR matrix.\n",
      "import numpy as np\n",
      "beta = np.ones(len(vocabulary))  # assume Beta = vector of 1s\n",
      "z = np.zeros(len(info1))\n",
      "for i in range(len(info1)):  # for each row.\n",
      "    for j in range(X.indptr[i], X.indptr[i+1]): # for each col.\n",
      "        colidx = X.indices[j]\n",
      "        z[i] += beta[colidx] * X.data[j]\n",
      "print z[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 13.  11.  18.  29.  34.]\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# y is a 1d numpy array of gender labels.\n",
      "# Let 1=Female, 0=Male.\n",
      "import numpy as np\n",
      "\n",
      "def get_gender(t, male_names, female_names):\n",
      "    name_label = get_first_name(t)\n",
      "    if name_label in female_names:\n",
      "        return 1\n",
      "    elif name_label in male_names:\n",
      "        return 0\n",
      "    else:\n",
      "        return -1\n",
      "    \n",
      "y = np.array([get_gender(t, male_names, female_names) for t in real_name])\n",
      "print 'gender labels:', Counter(y).items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gender labels: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 131151), (1, 199145)]\n"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Do 5-fold cross-validation\n",
      "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val(X, y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "acc= 0.744716924008\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.810881181974\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.793154604217\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.785661302775\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.815589094597\n",
        "0.790000621514\n"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tweet2tokens(info, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    tokens = []\n",
      "    if use_text:\n",
      "        tokens.extend(tokenize(info[1], lowercase, keep_punctuation, None,\n",
      "                           collapse_urls, collapse_mentions))\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(info[2], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_all(info, use_descr=True, lowercase=True,\n",
      "            keep_punctuation=True, descr_prefix=None,\n",
      "            collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    \n",
      "    tokens_list = [tweet2tokens(t, use_descr, lowercase,\n",
      "                            keep_punctuation, descr_prefix,\n",
      "                            collapse_urls, collapse_mentions, use_text)\n",
      "                  for t in info1]\n",
      "    vocabulary = make_vocabulary(tokens_list)\n",
      "    X = make_feature_matrix(tokens_list, vocabulary)\n",
      "    acc = do_cross_val(X, y, 5)\n",
      "    print 'acc=', acc\n",
      "    return acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_all(info1, use_descr=True, use_text=True)\n",
      "run_all(info1, use_descr=True, use_text=False)\n",
      "run_all(info1, use_descr=False, use_text=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "215094 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.770829548895\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.82235577287\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.801934634191\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.789566902315\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.832286289529\n",
        "acc= 0.80339462956\n",
        "146124 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.79485316379\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.843382430857\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.822673670507\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.807202652175\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.843972812183\n",
        "acc= 0.822416945902\n",
        "91978 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.609960641841\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.587641350914\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597148004057\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.585658275178\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.589230839098\n",
        "acc= 0.593927822218\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 153,
       "text": [
        "0.59392782221758778"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import product\n",
      "use_descr_opts = [True, False]\n",
      "lowercase_opts = [True, False]\n",
      "keep_punctuation_opts = [True, False]\n",
      "descr_prefix_opts = ['d=', '']\n",
      "url_opts = [True, False]\n",
      "mention_opts = [True, False]\n",
      "use_text_opts = [True, False]\n",
      "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention', 'use_text']\n",
      "option_iter = product(use_descr_opts, lowercase_opts,\n",
      "                       keep_punctuation_opts,\n",
      "                       descr_prefix_opts, url_opts,\n",
      "                       mention_opts, use_text_opts)\n",
      "results = []\n",
      "for options in option_iter:\n",
      "    if options[0] or options[-1]:  # skip options that don't use descr or text.\n",
      "        acc = run_all(info1, *options)\n",
      "        results.append((options, acc))\n",
      "best_result = max(results, key=lambda x: x[1])\n",
      "print 'best result:\\n\\tacc=%g\\n\\toptions=%s' % (best_result[1],\n",
      "                                                zip(argnames, best_result[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "238102 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.811239116894\n",
        "146124 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.822416945902\n",
        "268839 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.817003617721\n",
        "155361 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.824179002782\n",
        "265718 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.812407763201\n",
        "149285 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.823185953499\n",
        "296456 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.81754253206\n",
        "158522 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.824863238592\n",
        "215094 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.80339462956\n",
        "146124 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.822416945902\n",
        "245515 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.809077387297\n",
        "155361 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.824179002782\n",
        "242713 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.803885099787\n",
        "149285 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.823185953499\n",
        "273136 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.809555738568\n",
        "158522 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.824863238592\n",
        "111835 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.790000621514\n",
        "72594 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.799946181195\n",
        "138593 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.800703081568\n",
        "80947 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.804439123831\n",
        "139416 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.791759649972\n",
        "75799 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.800860513013\n",
        "166219 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.80041245329\n",
        "84155 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.805371620999\n",
        "96053 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.778949901971\n",
        "72594 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.799946181195\n",
        "122055 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.790394134544\n",
        "80947 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.804439123831\n",
        "123604 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.778389814531\n",
        "75799 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.800860513013\n",
        "149648 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.789749258863\n",
        "84155 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.805371620999\n",
        "283340 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.81605903024\n",
        "172422 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.827799988769\n",
        "314398 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.819967622133\n",
        "181862 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.829359195763\n",
        "310967 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.817291256763\n",
        "175583 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.828411562308\n",
        "342026 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.820415714579\n",
        "185023 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.829910217692\n",
        "256538 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.808947235294\n",
        "172422 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.827799988769\n",
        "287317 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.813309961947\n",
        "181862 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.829359195763\n",
        "284168 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.809631460609\n",
        "175583 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.828411562308\n",
        "314949 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.812816473931\n",
        "185023 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.829910217692\n",
        "146533 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.799973470124\n",
        "94230 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.810479152122\n",
        "173837 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.806921759779\n",
        "102960 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.813976017925\n",
        "174134 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.801762768085\n",
        "97436 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8112935733\n",
        "201483 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.806691670324\n",
        "106169 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.815099250101\n",
        "126173 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.789734155071\n",
        "94230 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.810479152122\n",
        "152855 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.797726960674\n",
        "102960 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.813976017925\n",
        "153759 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.789843153908\n",
        "97436 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8112935733\n",
        "180484 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.796948881058\n",
        "106169 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.815099250101\n",
        "91978 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.593927822218\n",
        "113478 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.604354742295\n",
        "116433 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-157-a4d95fd73e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moption_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# skip options that don't use descr or text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mbest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-151-05793fab23f3>\u001b[0m in \u001b[0;36mrun_all\u001b[0;34m(info, use_descr, lowercase, keep_punctuation, descr_prefix, collapse_urls, collapse_mentions, use_text)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_feature_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_cross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'acc='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-154-bcdb4309e9a3>\u001b[0m in \u001b[0;36mdo_cross_val\u001b[0;34m(X, y, nfolds)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/zhaoyixuan/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    701\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                                               rnd.randint(np.iinfo('i').max))\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}