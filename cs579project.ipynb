{
 "metadata": {
  "name": "",
  "signature": "sha256:134523179ae9fb94183e787cfd5bfde86f4acb368b0176ff25342733353c84c3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.Collect the data have \"#TheWalkingDead\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ConfigParser\n",
      "from TwitterAPI import TwitterAPI,TwitterRestPager\n",
      "import sys\n",
      "import time\n",
      "import ast,re\n",
      "\n",
      "def get_twitter(config_file):\n",
      "    config = ConfigParser.ConfigParser()\n",
      "    config.read(config_file)\n",
      "    twitter = TwitterAPI(\n",
      "                   config.get('twitter', 'consumer_key'),\n",
      "                   config.get('twitter', 'consumer_secret'),\n",
      "                   config.get('twitter', 'access_token'),\n",
      "                   config.get('twitter', 'access_token_secret'))\n",
      "    return twitter\n",
      "twitter = get_twitter('twitter1.cfg')\n",
      "print 'Established Twitter connection.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Established Twitter connection.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data(string,filename):\n",
      "\tpager = TwitterRestPager(twitter,'search/tweets',{'q':'#TheWalkingDead','count':200})\n",
      "\ti=0\n",
      "\tf=open(filename,'w')\n",
      "\tfor item in pager.get_iterator(new_tweets=True):\n",
      "\t\tf.write(str(item)+'\\n')\n",
      "\t\ti=i+1\n",
      "\t\tprint i\n",
      "\tf.close\n",
      "\n",
      "\n",
      "\n",
      "# get_data('#TheWalkingDead','TheWalkingDead2.txt')\n",
      "print 'ok'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data_positive():\n",
      "\tpager = TwitterRestPager(twitter,'statuses/user_timeline',{'screen_name':'thewalkingfans','count':200})\n",
      "\ti=0\n",
      "\tf=open('positive.txt','w')\n",
      "\tfor item in pager.get_iterator():\n",
      "\t\tf.write(str(item).encode('utf-8')+'\\n')\n",
      "\t\ti=i+1\n",
      "\t\tprint i\n",
      "\tf.close\n",
      "# get_data_positive()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_date(filename):\n",
      "\tcount=0\n",
      "\tf=open(filename,'r')\n",
      "\tfor i in f:\n",
      "\t\ti=ast.literal_eval(i)\n",
      "\t\tif i.has_key('created_at'):\n",
      "\t\t\tcount=count+1\n",
      "\t\t\tprint i['created_at']\n",
      "\t\t\tprint count\n",
      "\t\telse:\n",
      "\t\t\ti.clear()\n",
      "# get_date('TheWalkingDead.txt')\n",
      "# get_date('TheWalkingDead20.txt')\n",
      "# get_date('TheWalkingDead26.txt')\n",
      "# total have 414,700 data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Get data from txt file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# return a list of tuple that have user name, user text and user description\n",
      "import ast\n",
      "def get_des(filename):\n",
      "    f=open(filename,'r')\n",
      "    info=[]\n",
      "    for i in f:\n",
      "        i=ast.literal_eval(i)\n",
      "        if i['lang']==u'en':\n",
      "            info.append((i['user']['name'].encode('utf-8'),i['text'].encode('utf-8'),i['user']['description'].encode('utf-8')))\n",
      "    f.close()\n",
      "    return info\n",
      "# a=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead20.txt')\n",
      "# b=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead21-25.txt')\n",
      "# c=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead26.txt')\n",
      "# d=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead27-03.txt')\n",
      "info_pos=get_des('/Users/zhaoyixuan/Documents/cs579/positive.txt')\n",
      "b=get_des('/Users/zhaoyixuan/Documents/cs579/negative.txt')\n",
      "c=get_des('/Users/zhaoyixuan/Documents/cs579/negativefuck.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(info_pos),len(b),len(c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2905 394 1364\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add all the data into info list\n",
      "info_neg=[]\n",
      "count=0\n",
      "g=[b,c]\n",
      "for i in g:\n",
      "    for x in i:\n",
      "        info_neg.append(x)\n",
      "        count+=1\n",
      "print count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1758\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of corresponding name, text and description\n",
      "name_test=[]\n",
      "text_test=[]\n",
      "description_test=[]\n",
      "for i in info:\n",
      "    name_test.append(i[0])\n",
      "    text_test.append(i[1])\n",
      "    description.append(i[2])\n",
      "print 'Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(info, open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'wb'))\n",
      "pickle.dump(name, open('/Users/zhaoyixuan/Documents/cs579/name_test.pkl', 'wb'))\n",
      "pickle.dump(text, open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'wb'))\n",
      "pickle.dump(description, open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'wb'))\n",
      "pickle.dump(info1,open('/Users/zhaoyixuan/Documents/cs579/info1.pkl', 'wb'))\n",
      "pickle.dump(test_tweet,open('/Users/zhaoyixuan/Documents/cs579/test_tweet.pkl', 'wb'))\n",
      "pickle.dump(test_label,open('/Users/zhaoyixuan/Documents/cs579/test_label.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "# info=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'rb'))\n",
      "name=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/name.pkl', 'rb'))\n",
      "# text=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'rb'))\n",
      "# description=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'rb'))\n",
      "info1=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/info1.pkl', 'rb'))\n",
      "test_tweet=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/test_tweet.pkl', 'rb'))\n",
      "test_label=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/test_label.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print info_pos[0]\n",
      "print info_neg[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('The Walking Dead', \"RT @IGN: Walking Dead producer delves into Rick &amp; Daryl's new dynamic. Plus, why'd Sasha do THAT? http://t.co/gzVcmnt3aY http://t.co/yIjjYF\\xe2\\x80\\xa6\", 'Sundays @ 9|8c. Only on @AMC_TV.')\n",
        "('\\xe2\\xad\\x90Starla\\xe2\\xad\\x90', \"I can't believe there is only 1 more new The Walking Dead episode this year. #sucks\", 'Loves:  Music, travel, The Walking Dead, Game of Thrones.')\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.Filter data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fetch male/female names from Census.\n",
      "import requests\n",
      "\n",
      "def get_census_names():\n",
      "    \"\"\" Fetch a list of common male/female names from the census.\n",
      "    For ambiguous names, we select the more frequent gender.\"\"\"\n",
      "    males = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first').text.split('\\n')\n",
      "    females = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first').text.split('\\n')\n",
      "    males_pct = dict([(m.split()[0].lower(), float(m.split()[1]))\n",
      "                  for m in males if m])\n",
      "    females_pct = dict([(f.split()[0].lower(), float(f.split()[1]))\n",
      "                    for f in females if f])\n",
      "    male_names = set([m for m in males_pct if m not in females_pct or\n",
      "                  males_pct[m] > females_pct[m]])\n",
      "    female_names = set([f for f in females_pct if f not in males_pct or\n",
      "                  females_pct[f] > males_pct[f]])    \n",
      "    return male_names, female_names\n",
      "\n",
      "male_names, female_names = get_census_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "from collections import Counter\n",
      "def get_first_name(t):\n",
      "    parts =t.split()\n",
      "    if len(parts)>0:\n",
      "        return parts[0].lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter the data and keep the first name in male_names of female_names\n",
      "def sample(name_test, male_names, female_names):\n",
      "    count=0\n",
      "    real_name=[]\n",
      "    num=[]\n",
      "    i=0\n",
      "    for t in name_test:\n",
      "        name = get_first_name(t)\n",
      "        if name in male_names:\n",
      "            real_name.append(name)\n",
      "            num.append(i)\n",
      "        if name in  female_names:\n",
      "            real_name.append(name)\n",
      "            num.append(i)\n",
      "        i+=1\n",
      "    return real_name,num"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "real_name,num=sample(name,male_names,female_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# info1 store the filtered data\n",
      "info1=[]\n",
      "for i in num:\n",
      "    info1.append(info[i])\n",
      "print len(info1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'info' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-11-85cc11030f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minfo1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minfo1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'info' is not defined"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print real_name[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['chad', 'sammy', 'katie', 'abraham', 'fernando', 'tammie', 'katherine', 'angela', 'raven', 'holly', 'sam', 'tammie', 'jessica', 'kiara', 'ana', 'tatiana', 'daryl', 'dana', 'max', 'cathy']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Sentiment analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from StringIO import StringIO\n",
      "from zipfile import ZipFile\n",
      "from urllib import urlopen\n",
      "\n",
      "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
      "zipfile = ZipFile(StringIO(url.read()))\n",
      "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
      "\n",
      "afinn = dict()\n",
      "\n",
      "for line in afinn_file:\n",
      "    parts = line.strip().split()\n",
      "    if len(parts) == 2:\n",
      "        afinn[parts[0]] = int(parts[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score=[]\n",
      "for tweet in info_pos:\n",
      "    score.append((' '.join(tweet[1].split()),4))\n",
      "for tweet in info_neg:\n",
      "    score.append((' '.join(tweet[1].split()),2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(info_pos),len(info_neg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2905 1758\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "import numpy as np\n",
      "y=np.array([t[1] for t in score])\n",
      "print 'label counts=', Counter(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "label counts= Counter({4: 2905, 2: 1758})\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(t[0] for t in score)\n",
      "print 'vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vectorized 4663 tweets. found 7770 terms.\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Baseline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.dummy import DummyClassifier\n",
      "def do_baseline(X,y):\n",
      "    clf=DummyClassifier(strategy='most_frequent',random_state=0)\n",
      "    clf.fit(X,y)\n",
      "    return(clf.score(X,y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The accuracy of the baseline is:',do_baseline(X,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The accuracy of the baseline is: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.622989491744\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression(C=100.0)\n",
      "model.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# LogisticRegresstion\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "def do_cross_val_LR(C_var,X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression(C=C_var)\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C_var=[0.01,0.1,0.5,1.0,5.0,10.0,100.0,1000.0]\n",
      "result_LR=[]\n",
      "for i in C_var:\n",
      "    avg=do_cross_val_LR(i,X,y,5)\n",
      "    result_LR.append(avg)\n",
      "    print 'avg accuracy', i,avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy 0.01 0.902164092939\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.1 0.908160026496\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.5 0.911590052855\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.0 0.912019237404\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.0 0.914379522423\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10.0 0.914808706972\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100.0 0.926612202089\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000.0 0.93391040945\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "When C=1000.0, it has the highest accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "C_var=[0.01,0.1,0.5,1.0,5.0,10.0,100.0,1000.0]\n",
      "plt.figure()\n",
      "plt.title('Accuracy of Logistic Regression')\n",
      "plt.plot(result_LR, 'bo-')\n",
      "plt.xlabel('C')\n",
      "plt.ylabel('avg accuracy')\n",
      "plt.savefig('Logistic_accuracy.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SVM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "clf = svm.SVC(C=1000.0)\n",
      "clf.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "SVC(C=1000.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
        "  gamma=0.0, kernel='rbf', max_iter=-1, probability=False,\n",
        "  random_state=None, shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# svm\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "def do_cross_val_svm(C_var,X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf =svm.SVC(C=C_var)\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C_var=[0.01,0.1,0.5,1.0,5.0,10.0,100.0,1000.0]\n",
      "result_svm=[]\n",
      "for i in C_var:\n",
      "    avg=do_cross_val_svm(i,X,y,5)\n",
      "    result_svm.append(avg)\n",
      "    print 'avg accuracy', i,avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy 0.01 0.622746781116\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.1 0.622746781116\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.5 0.622746781116\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.0 0.603455326626\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.0 0.887582628376\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10.0 0.890799672477\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100.0 0.912019237404\n",
        "avg accuracy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000.0 0.913520693319\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "When C=1000.0, it has the highest accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "C_var=[0.01,0.1,0.5,1.0,5.0,10.0,100.0,1000.0]\n",
      "plt.figure()\n",
      "plt.title('Accuracy of svm')\n",
      "plt.plot(result_svm, 'bo-')\n",
      "plt.xlabel('C')\n",
      "plt.ylabel('avg accuracy')\n",
      "plt.savefig('svm_accuracy.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gaussian Naive Bayes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "gnb = GaussianNB()\n",
      "gnb.fit(X.todense(),y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "GaussianNB()"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# GaussianNB\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "def do_cross_val_GNB(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf =GaussianNB()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val_GNB(X.todense(),y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.692262027977\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Predicate label"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.linear_model\n",
      "from sklearn.svm import SVC \n",
      "vectorizer1 = CountVectorizer()\n",
      "list_test=[(1,7715),(7716,30390),(30391,41482),(41483,49750),(49762,59650),(59660,66236),(66237,79268),(79269,94460),(94461,108065)]\n",
      "def predicate(model,pred):\n",
      "    for i in list_test:\n",
      "        X2 = vectorizer1.fit_transform(t[1] for t in info1[i[0]:i[1]])\n",
      "        for num in model.predict(X2).tolist():\n",
      "            pred.append(num)\n",
      "    return pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicate_LR=predicate(model,[])\n",
      "print 'finish Logistic Regression'\n",
      "predicate_svm=predicate(clf,[])\n",
      "print 'finish svm'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "finish Logistic Regression\n",
        "finish svm"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.linear_model\n",
      "from sklearn.svm import SVC \n",
      "vectorizer1 = CountVectorizer()\n",
      "list_test=[(1,7715),(7716,30390),(30391,41482),(41483,49750),(49762,59650),(59660,66236),(66237,79268),(79269,94460),(94461,108065)]\n",
      "def predicate(model,pred):\n",
      "    for i in list_test:\n",
      "        X2 = vectorizer1.fit_transform(t[1] for t in info1[i[0]:i[1]])\n",
      "        for num in model.predict(X2.todense()).tolist():\n",
      "            pred.append(num)\n",
      "    return pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicate_GNB=predicate(gnb,[])\n",
      "print 'finish GNB'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "finish GNB\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_tweet=[]\n",
      "real_name_test=[]\n",
      "for i in list_test:\n",
      "    a=i[0]\n",
      "    for a in range(a,i[1]):\n",
      "        test_tweet.append(info1[a])\n",
      "        real_name_test.append(info1[a][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Logistic Regression:',Counter(predicate_LR)\n",
      "print 'svm:',Counter(predicate_svm)\n",
      "print 'Gaussian Naive Bayes:',Counter(predicate_GNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Logistic Regression: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({4: 107846, 2: 190})\n",
        "svm: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({4: 107847, 2: 189})\n",
        "Gaussian Naive Bayes: Counter({4: 100709, 2: 7327})\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Use Affin sentiment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def afinn_sentiment(terms, afinn):\n",
      "    total = 0\n",
      "    for t in terms.split():\n",
      "        if t in afinn:\n",
      "            total += afinn[t]\n",
      "    return total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# afinn test tweet\n",
      "afinn_score_test=[]\n",
      "for tweet in test_tweet:\n",
      "    total=afinn_sentiment(tweet[1],afinn)\n",
      "    if total>0:\n",
      "        total=4\n",
      "    elif total<0:\n",
      "        total=2\n",
      "    afinn_score_test.append(total)\n",
      "print Counter(afinn_score_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({0: 68399, 4: 26162, 2: 13475})\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# afinn all info1 tweet\n",
      "afinn_score_all=[]\n",
      "for tweet in info1:\n",
      "    total=afinn_sentiment(tweet[1],afinn)\n",
      "    if total>0:\n",
      "        total=4\n",
      "    elif total<0:\n",
      "        total=2\n",
      "    afinn_score_all.append(total)\n",
      "print Counter(afinn_score_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({0: 216763, 4: 62836, 2: 50697})\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.Gender prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(string, lowercase, keep_punctuation, prefix,\n",
      "             collapse_urls, collapse_mentions):\n",
      "    if not string:\n",
      "        return []\n",
      "    if lowercase:\n",
      "        string = string.lower()\n",
      "    tokens = []\n",
      "    if collapse_urls:\n",
      "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
      "    if collapse_mentions:\n",
      "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
      "    if keep_punctuation:\n",
      "        tokens = string.split()\n",
      "    else:\n",
      "        tokens = re.sub('\\W+', ' ', string).split()\n",
      "    if prefix:\n",
      "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
      "    return tokens\n",
      "\n",
      "def tweet2tokens(tweet, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    tokens = []\n",
      "    if use_text:\n",
      "        tokens.extend(tokenize(tweet[1], lowercase, keep_punctuation, None,\n",
      "                           collapse_urls, collapse_mentions))\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(tweet[2], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's tokenize all tweets.\n",
      "tokens_list_test = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=False, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True)\n",
      "              for t in test_tweet]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(tokens_list_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "108036\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "def make_vocabulary(tokens_list):\n",
      "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
      "    for tokens in tokens_list:\n",
      "        for token in tokens:\n",
      "            vocabulary[token]\n",
      "    print '%d unique terms in vocabulary' % len(vocabulary)\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary_test = make_vocabulary(tokens_list_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "64622 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.sparse import lil_matrix\n",
      "\n",
      "def make_feature_matrix(tokens_list, vocabulary,i):\n",
      "    X = lil_matrix((i, len(vocabulary)))\n",
      "    for i, tokens in enumerate(tokens_list):\n",
      "        for token in tokens:\n",
      "            j = vocabulary[token]\n",
      "            X[i,j] += 1\n",
      "    return X.tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test = make_feature_matrix(tokens_list_test, vocabulary_test,len(test_tweet))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'shape of X:', X_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shape of X: (108036, 64622)\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let 1=Female, 0=Male.\n",
      "import numpy as np\n",
      "\n",
      "def get_gender(t, male_names, female_names):\n",
      "    name_label = get_first_name(t)\n",
      "    if name_label in female_names:\n",
      "        return 1\n",
      "    elif name_label in male_names:\n",
      "        return 0\n",
      "    else:\n",
      "        return -1\n",
      "    \n",
      "y_test = np.array([get_gender(t, male_names, female_names) for t in real_name_test])\n",
      "print 'test gender labels:', Counter(y_test).items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test gender labels: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 40560), (1, 67476)]\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'The accuracy of all tweet:',do_baseline(X_test,y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The accuracy of all tweet: 0.624569587915\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# afinn_score_test and afinn_score_all\n",
      "def compare(sentiments,gender):\n",
      "    dic={'mp':0,'mn':0,'fp':0,'fn':0}\n",
      "    a=len(sentiments)\n",
      "    for i in range(0,a-1):\n",
      "        if sentiments[i]==4 and gender[i]==0:\n",
      "            dic['mp']+=1\n",
      "        elif sentiments[i]==2 and gender[i]==0:\n",
      "            dic['mn']+=1\n",
      "        elif sentiments[i]==4 and gender[i]==1:\n",
      "            dic['fp']+=1\n",
      "        elif sentiments[i]==2 and gender[i]==1:\n",
      "            dic['fn']+=1\n",
      "    return dic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dic_test=compare(afinn_score_test,y_test)\n",
      "print dic_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'fp': 16964, 'mn': 5549, 'mp': 9198, 'fn': 7926}\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "plt.figure()\n",
      "plt.title('compare of test')\n",
      "plt.xticks([0,1], ['female','male'])\n",
      "p1=plt.bar(np.arange(2),[dic_test['fp']+dic_test['fn'],0],0.35, color='b')\n",
      "p2=plt.bar(np.arange(2)+1.0,[dic_test['mp']+dic_test['mn'],0],0.35,color= 'r')\n",
      "# plt.xlabel(dic_test.keys())\n",
      "# plt.ylabel(dic_test.values())\n",
      "plt.savefig('compare of test.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "plt.figure()\n",
      "plt.title('compare of all')\n",
      "plt.xticks([0,1], ['female','male'])\n",
      "p1=plt.bar(np.arange(2),[dic_all['fp']+dic_all['fn'],0],0.35, color='b')\n",
      "p2=plt.bar(np.arange(2)+1.0,[dic_all['mp']+dic_all['mn'],0],0.35,color= 'r')\n",
      "# plt.xlabel(dic_all.keys())\n",
      "# plt.ylabel(dic_all.values())\n",
      "plt.savefig('compare of all.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare1(dic):\n",
      "    male={'positive':0,'negative':0}\n",
      "    female={'positive':0,'negative':0}\n",
      "    male['positive']=dic['mp']\n",
      "    male['negative']=dic['mn']\n",
      "    female['positive']=dic['fp']\n",
      "    female['negative']=dic['fn']\n",
      "    return male,female"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "male_test,female_test=compare1(dic_test)\n",
      "male_all,female_all=compare1(dic_all)\n",
      "print male_test,female_test\n",
      "print male_all,female_all"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'positive': 9198, 'negative': 5549} {'positive': 16964, 'negative': 7926}\n",
        "{'positive': 23849, 'negative': 20981} {'positive': 38986, 'negative': 29716}\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "plt.figure()\n",
      "plt.title('compare of test')\n",
      "fig, ax1 = plt.subplots()\n",
      "ax1.plot(male_test.values(), 'bo-', label='male')\n",
      "ax1.set_ylabel('male', color='b')\n",
      "plt.xticks([4,2], ['positive','negative'])\n",
      "ax2 = ax1.twinx()\n",
      "ax2.plot(female_test.values(), 'ro-', label='female')\n",
      "ax2.set_ylabel('female', color='r')\n",
      "plt.savefig('compare test.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "plt.figure()\n",
      "plt.title('compare of all')\n",
      "fig, ax1 = plt.subplots()\n",
      "ax1.plot(male_all.values(), 'bo-', label='male')\n",
      "plt.xticks([4,2], ['positive','negative'])\n",
      "ax1.set_ylabel('male', color='b')\n",
      "ax2 = ax1.twinx()\n",
      "ax2.plot(female_all.values(), 'ro-', label='female')\n",
      "ax2.set_ylabel('female', color='r')\n",
      "plt.savefig('compare all.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_all(test_tweet, use_descr=True, lowercase=True,\n",
      "            keep_punctuation=True, descr_prefix=None,\n",
      "            collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    \n",
      "    tokens_list = [tweet2tokens(t, use_descr, lowercase,\n",
      "                            keep_punctuation, descr_prefix,\n",
      "                            collapse_urls, collapse_mentions, use_text)\n",
      "                  for t in test_tweet]\n",
      "    vocabulary = make_vocabulary(tokens_list)\n",
      "    X = make_feature_matrix(tokens_list, vocabulary,len(test_tweet))\n",
      "    acc = do_cross_val(X, y_test, 5)\n",
      "    print 'acc=', acc\n",
      "    return acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_all(test_tweet, use_descr=True, use_text=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "113036 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.745353158087\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 167,
       "text": [
        "0.74535315808685498"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_all(test_tweet, use_descr=True, use_text=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "84087 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.75453537955\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 168,
       "text": [
        "0.75453537955042105"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_all(test_tweet, use_descr=False, use_text=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "41323 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.625624151106\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 169,
       "text": [
        "0.62562415110615821"
       ]
      }
     ],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize2(test_tweet,vectorizer):\n",
      "    X=vectorizer.fit_transform(t[2] for t in test_tweet)\n",
      "    print 'X dimensions=',X.shape\n",
      "    return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X=tokenize2(test_tweet,vectorizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "X dimensions= (108036, 51570)\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "def do_cv(X, y_test, nfolds=5):\n",
      "    cv = KFold(len(y_test), nfolds)\n",
      "    accuracy=[]\n",
      "    for i in C_var:\n",
      "        accuracy.append(np.mean(cross_val_score(LogisticRegression(C=i), X, y_test, cv=cv)))\n",
      "    return accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare_mindf(test_tweet,freq,y_test):\n",
      "    return do_cv(tokenize2(test_tweet, TfidfVectorizer(min_df=freq)), y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracies=[]\n",
      "min_df=[2,4,6,8,10]\n",
      "for i in min_df:\n",
      "    print i\n",
      "    accuracies.append(compare_mindf(test_tweet,i,y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 27633)\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 16271)\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 12151)\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 9929)\n",
        "10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 8609)\n"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "C_var=[0.01,0.1,0.5,1.0,5.0,10.0]\n",
      "def figure(i,name):\n",
      "    plt.figure()\n",
      "    plt.title('Accuracy of Logistic Regression')\n",
      "    plt.plot(C_var,i, 'bo-')\n",
      "    plt.xlabel('C')\n",
      "    plt.ylabel('avg accuracy')\n",
      "    plt.savefig(name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for a,b in zip(accuracies,min_df):\n",
      "    figure(a,'Logistic_accuracy_mindf'+str(b)+'.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare_maxdf(test_tweet,freq,y_test):\n",
      "    return do_cv(tokenize2(test_tweet, TfidfVectorizer(max_df=freq)), y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracies1=[]\n",
      "maxdf = [.005, .001,.0005,.0001,.00005,.00001]\n",
      "for i in maxdf[::-1]:\n",
      "    print i\n",
      "    accuracies1.append(compare_maxdf(test_tweet,i,y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1e-05\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 23937)\n",
        "5e-05"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 39419)\n",
        "0.0001"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 43458)\n",
        "0.0005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 49187)\n",
        "0.001"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 50264)\n",
        "0.005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 51292)\n"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print accuracies1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0.62456910501137364, 0.62456910501137364, 0.62456910501137364, 0.62456910501137364, 0.62456910501137364, 0.62456910501137364], [0.62456910501137364, 0.62456910501137364, 0.63065039813653434, 0.64167451286877963, 0.65543851126927322, 0.65603090245600504], [0.62456910501137364, 0.62456910501137364, 0.64336842036002184, 0.65542926314879346, 0.67313636471663962, 0.67418231990342381], [0.62456910501137364, 0.63741669430664816, 0.68152257013462891, 0.69221349464972948, 0.69982205817711618, 0.70270070477626256], [0.62456910501137364, 0.649625644102098, 0.69257447163712949, 0.70429283153760125, 0.71194769906642519, 0.71352126447427877], [0.62809571291078736, 0.6827534791635721, 0.72135188184708543, 0.7304044706979671, 0.73750403719076674, 0.7371337829535991]]\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for a,b in zip(accuracies1,maxdf):\n",
      "    figure(a,'Logistic_accuracy_maxdf'+str(b)+'.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare_ngram(test_tweet,freq,y_test):\n",
      "    return do_cv(tokenize2(test_tweet, TfidfVectorizer(ngram_range=freq)), y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracies2 = []\n",
      "ngrams = [(1,1), (1,2), (2,2), (1,3), (2, 3), (3,3)]\n",
      "for i in ngrams:\n",
      "    print i\n",
      "    accuracies2.append(compare_ngram(test_tweet,i,y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1, 1)\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 51570)\n",
        "(1, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 294843)\n",
        "(2, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 243273)\n",
        "(1, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 631762)\n",
        "(2, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 580192)\n",
        "(3, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (108036, 336919)\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print accuracies2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0.63001171708463732, 0.70287649831417665, 0.72896048307119743, 0.73827219964007507, 0.74768582030995723, 0.74843557304942532], [0.62667951805147948, 0.68732608165998277, 0.74034561848477431, 0.75664580501331158, 0.77367717650225354, 0.77424182461266144], [0.62672578649805089, 0.65303192019904821, 0.7115681757185558, 0.73515295797550206, 0.74887989021411339, 0.74936121099838959], [0.6270590246928458, 0.67372874947779327, 0.7395958717425124, 0.75715489800319935, 0.77592645828111129, 0.77576908302047465], [0.62672578649805089, 0.64987554897355504, 0.7051906685800382, 0.7327000976019622, 0.74759328812890546, 0.74856518852723464], [0.6264666112307764, 0.64686727790079002, 0.69431464671819942, 0.72305515997316483, 0.73068227800008667, 0.73170046012451551]]\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for a,b in zip(accuracies2,maxdf):\n",
      "    figure(a,'Logistic_accuracy_ngram'+str(b)+'.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}