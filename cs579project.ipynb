{
 "metadata": {
  "name": "",
  "signature": "sha256:1fb06d8e2b6be47806057086d0079aa6134baa0d7a7925113ecb22c8118fef8f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ConfigParser\n",
      "from TwitterAPI import TwitterAPI,TwitterRestPager\n",
      "import sys\n",
      "import time\n",
      "import ast,re\n",
      "\n",
      "def get_twitter(config_file):\n",
      "    config = ConfigParser.ConfigParser()\n",
      "    config.read(config_file)\n",
      "    twitter = TwitterAPI(\n",
      "                   config.get('twitter', 'consumer_key'),\n",
      "                   config.get('twitter', 'consumer_secret'),\n",
      "                   config.get('twitter', 'access_token'),\n",
      "                   config.get('twitter', 'access_token_secret'))\n",
      "    return twitter\n",
      "twitter = get_twitter('twitter1.cfg')\n",
      "print 'Established Twitter connection.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Established Twitter connection.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_data(string,filename):\n",
      "\tpager = TwitterRestPager(twitter,'search/tweets',{'q':'#TheWalkingDead','count':200})\n",
      "\ti=0\n",
      "\tf=open(filename,'w')\n",
      "\tfor item in pager.get_iterator(new_tweets=True):\n",
      "\t\tf.write(str(item)+'\\n')\n",
      "\t\ti=i+1\n",
      "\t\tprint i\n",
      "\tf.close\n",
      "\n",
      "\n",
      "\n",
      "# get_data('#TheWalkingDead','TheWalkingDead2.txt')\n",
      "print 'ok'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ok\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_date(filename):\n",
      "\tcount=0\n",
      "\tf=open(filename,'r')\n",
      "\tfor i in f:\n",
      "\t\ti=ast.literal_eval(i)\n",
      "\t\tif i.has_key('created_at'):\n",
      "\t\t\tcount=count+1\n",
      "\t\t\tprint i['created_at']\n",
      "\t\t\tprint count\n",
      "\t\telse:\n",
      "\t\t\ti.clear()\n",
      "# get_date('TheWalkingDead.txt')\n",
      "# get_date('TheWalkingDead20.txt')\n",
      "# get_date('TheWalkingDead26.txt')\n",
      "# total have 414,700 data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ast\n",
      "def get_des(filename):\n",
      "    f=open(filename,'r')\n",
      "    info=[]\n",
      "    for i in f:\n",
      "        i=ast.literal_eval(i)\n",
      "        if i['lang']==u'en':\n",
      "            info.append((i['user']['name'].encode('utf-8'),i['text'].encode('utf-8'),i['user']['description'].encode('utf-8')))\n",
      "    f.close()\n",
      "    return info\n",
      "# a=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead20.txt')\n",
      "# b=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead21-25.txt')\n",
      "# c=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead26.txt')\n",
      "d=get_des('/Users/zhaoyixuan/Documents/cs579/data/TheWalkingDead27-03.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(a),len(b),len(c),len(d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "79514 57728 19491 535787\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "info=[]\n",
      "count=0\n",
      "g=[a,b,c,d]\n",
      "for i in g:\n",
      "    for x in i:\n",
      "        info.append(x)\n",
      "        count+=1\n",
      "print count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "692520\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name=[]\n",
      "text=[]\n",
      "description=[]\n",
      "for i in info:\n",
      "    name.append(i[0])\n",
      "    text.append(i[1])\n",
      "    description.append(i[2])\n",
      "print 'Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(info, open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'wb'))\n",
      "pickle.dump(name, open('/Users/zhaoyixuan/Documents/cs579/name.pkl', 'wb'))\n",
      "pickle.dump(text, open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'wb'))\n",
      "pickle.dump(description, open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "info=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/info.pkl', 'rb'))\n",
      "name=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/name.pkl', 'rb'))\n",
      "text=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/text.pkl', 'rb'))\n",
      "description=pickle.load(open('/Users/zhaoyixuan/Documents/cs579/description.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(info),len(name),len(text),len(description)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "692520 692520 692520 692520\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from StringIO import StringIO\n",
      "from zipfile import ZipFile\n",
      "from urllib import urlopen\n",
      "\n",
      "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
      "zipfile = ZipFile(StringIO(url.read()))\n",
      "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
      "\n",
      "afinn = dict()\n",
      "\n",
      "for line in afinn_file:\n",
      "    parts = line.strip().split()\n",
      "    if len(parts) == 2:\n",
      "        afinn[parts[0]] = int(parts[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'read', len(afinn), 'AFINN terms.\\nE.g.:', afinn.items()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 2462 AFINN terms.\n",
        "E.g.: [('limited', -1), ('suicidal', -2), ('pardon', 2), ('desirable', 2), ('protest', -2)]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def afinn_sentiment(terms, afinn):\n",
      "    total = 0.\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "#             print '\\t%s=%d' % (t, afinn[t])\n",
      "            total += afinn[t]\n",
      "    return total\n",
      "# a=[]\n",
      "# for tweet in text:\n",
      "#     doc=tweet.split()\n",
      "#     t=afinn_sentiment(doc,afinn)\n",
      "#     a.append(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def afinn_sentiment2(terms, afinn, verbose=False):\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            if verbose:\n",
      "                print '\\t%s=%d' % (t, afinn[t])\n",
      "            if afinn[t] > 0:\n",
      "                pos += afinn[t]\n",
      "            else:\n",
      "                neg += 1 * afinn[t]\n",
      "    return pos, neg\n",
      "\n",
      "doc = text[0].split()\n",
      "print 'AFINN:\\n', afinn_sentiment2(doc, afinn, verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AFINN:\n",
        "(0, 0)\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "positives = []\n",
      "negatives = []\n",
      "other=[]\n",
      "total=[]\n",
      "non=[]\n",
      "i=0\n",
      "for tweet in text:\n",
      "    i=i+1\n",
      "    pos, neg = afinn_sentiment2(tweet, afinn)\n",
      "    if pos > -1*neg:\n",
      "        total.append((''.join(tweet),pos))\n",
      "        positives.append((''.join(tweet), pos, neg))\n",
      "    elif -1*neg > pos:\n",
      "        total.append((''.join(tweet),neg))\n",
      "        negatives.append((''.join(tweet), pos, neg))\n",
      "    elif neg==pos:\n",
      "        total.append((''.join(tweet),0))\n",
      "        other.append((''.join(tweet),pos,neg))\n",
      "    else:\n",
      "        non.append((''.join(tweet),pos,neg))\n",
      "print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "692520\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print positives[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[]\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total=[]\n",
      "for tweet in info1:\n",
      "    tweet=tweet[1].split()\n",
      "    t=afinn_sentiment(tweet,afinn)\n",
      "    total.append((' '.join(tweet),t))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(total)\n",
      "print total[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "330296\n",
        "[(\"Coach Muschamp is the patient zero of #TheWalkingDead cause he's a zombie\", 0.0), ('bloody hell that ending was a bit grim #TheWalkingDead', -7.0), ('RT @n1ghtlock: THIS IS TOO MUCH #TheWalkingDeadUK #TheWalkingDead http://t.co/xZ9JAarXpw', 0.0), ('@BAMFAbraham ^^^^^^^^^^ I get the bus running and look what happens. #smh #TheWalkingDead', 0.0), (\"I'm watching #TheWalkingDead with 94 others on #tvtag http://t.co/k1y4IE4AUD\", 0.0), ('RT @TheWalkingNews: Best.Selfie.Ever. #TheWalkingDead http://t.co/7FhZhwBLzC', 0.0), (\"RT @WalkingDead_AMC: Stay off the roads. Stay in the forest and go inside last night's #TheWalkingDead. http://t.co/7w3xnTFWpQ\", 0.0), ('watched #TheWalkingDead s05e02 before breakfast/going to work \\xf0\\x9f\\x98\\x8a\\xf0\\x9f\\x91\\xa4\\xf0\\x9f\\x91\\xa4\\xf0\\x9f\\x91\\xa3\\xf0\\x9f\\x91\\xa3', 0.0), (\"RT @WalkingDead_AMC: Stay off the roads. Stay in the forest and go inside last night's #TheWalkingDead. http://t.co/7w3xnTFWpQ\", 0.0), ('RT @tvbinges: @hwilson2009 @zorash1330 @Rosa_Austin1996 fan of #TheWalkingDead? Join #TWDBinge Sat Nov 8th http://t.co/dxdfqftRtS #TWDFamil\\xe2\\x80\\xa6', 3.0)]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sorted(positives,key=lambda positives:positives[1],reverse=True)[:6]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[]\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "import numpy as np\n",
      "y = np.array([t[1] for t in total])\n",
      "print 'label counts=', Counter(y)\n",
      "print len(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "label counts= "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counter({0.0: 216763, 2.0: 18572, 3.0: 14373, -3.0: 13557, -2.0: 13309, 4.0: 10218, 1.0: 9182, -1.0: 9033, 6.0: 8754, -4.0: 8637, -5.0: 1968, -6.0: 1752, -7.0: 1276, 5.0: 1231, -8.0: 547, -9.0: 232, 7.0: 215, -10.0: 203, 8.0: 136, 9.0: 69, -11.0: 59, -12.0: 58, 10.0: 52, -14.0: 22, -13.0: 22, 11.0: 20, 12.0: 7, -16.0: 7, -17.0: 5, 13.0: 4, -15.0: 3, -21.0: 2, -18.0: 2, 14.0: 1, 15.0: 1, 27.0: 1, -32.0: 1, -22.0: 1, -19.0: 1})\n",
        "330296\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(t[1] for t in info1)\n",
      "print 'vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vectorized 330296 tweets. found 82044 terms.\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def accuracy(truth, predicted):\n",
      "    return (1. * len([1 for tr, pr in zip(truth, predicted) if tr == pr]) / len(truth))\n",
      "\n",
      "predicted = model.predict(X)\n",
      "print 'accuracy on training data=%.3f' % accuracy(y, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy on training data=0.893\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 5 Cross-validation accuracy\n",
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "cv = KFold(len(y), 5)\n",
      "accuracies = []\n",
      "for train_ind, test_ind in cv:\n",
      "    model.fit(X[train_ind], y[train_ind])\n",
      "    predictions = model.predict(X[test_ind])\n",
      "    accuracies.append(accuracy(y[test_ind], predictions))\n",
      "    \n",
      "print 'Average 5-fold cross validation accuracy=%.2f (std=%.2f)' % (np.mean(accuracies), np.std(accuracies))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fetch male/female names from Census.\n",
      "\n",
      "import requests\n",
      "\n",
      "def get_census_names():\n",
      "    \"\"\" Fetch a list of common male/female names from the census.\n",
      "    For ambiguous names, we select the more frequent gender.\"\"\"\n",
      "    males = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first').text.split('\\n')\n",
      "    females = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first').text.split('\\n')\n",
      "    males_pct = dict([(m.split()[0].lower(), float(m.split()[1]))\n",
      "                  for m in males if m])\n",
      "    females_pct = dict([(f.split()[0].lower(), float(f.split()[1]))\n",
      "                    for f in females if f])\n",
      "    male_names = set([m for m in males_pct if m not in females_pct or\n",
      "                  males_pct[m] > females_pct[m]])\n",
      "    female_names = set([f for f in females_pct if f not in males_pct or\n",
      "                  females_pct[f] > males_pct[f]])    \n",
      "    return male_names, female_names\n",
      "\n",
      "male_names, female_names = get_census_names()\n",
      "print 'male name sample:', list(male_names)[:5]\n",
      "print 'female name sample:', list(female_names)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "male name sample: [u'trenton', u'darrin', u'emile', u'jason', u'ron']\n",
        "female name sample: [u'fawn', u'kymberly', u'augustina', u'evalyn', u'chieko']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "from collections import Counter\n",
      "def get_first_name(t):\n",
      "#     first_name=[]\n",
      "    parts =t.split()\n",
      "    if len(parts) > 0:\n",
      "        return parts[0].lower()\n",
      "def sample(name, male_names, female_names):\n",
      "    i=0\n",
      "    real_name=[]\n",
      "    num=[]\n",
      "    for t in name:\n",
      "        name = get_first_name(t)\n",
      "        if name in male_names or name in female_names:\n",
      "            real_name.append(name)\n",
      "            num.append(i)\n",
      "        i+=1\n",
      "    i=0\n",
      "    for t in name:\n",
      "        if t not in real_name:\n",
      "            a=re.sub('\\W+',' ',t)\n",
      "            a=a.split()\n",
      "            for i in range(0,len(a)-1):\n",
      "                if a[i] in male_names or a[i] in female_names:\n",
      "                    real_name.append(t)\n",
      "                    num.append(i)\n",
      "        i+=1\n",
      "    return real_name,num\n",
      "real_name,num=sample(name,male_names,female_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "info1=[]\n",
      "for i in num:\n",
      "    info1.append(info[i])\n",
      "print len(info1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "330296\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(string, lowercase, keep_punctuation, prefix,\n",
      "             collapse_urls, collapse_mentions):\n",
      "    if not string:\n",
      "        return []\n",
      "    if lowercase:\n",
      "        string = string.lower()\n",
      "    tokens = []\n",
      "    if collapse_urls:\n",
      "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
      "    if collapse_mentions:\n",
      "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
      "    if keep_punctuation:\n",
      "        tokens = string.split()\n",
      "    else:\n",
      "        tokens = re.sub('\\W+', ' ', string).split()\n",
      "    if prefix:\n",
      "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
      "    return tokens\n",
      "\n",
      "def tweet2tokens(infomation, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True):\n",
      "    tokens = tokenize(infomation[1], lowercase, keep_punctuation, None,\n",
      "                       collapse_urls, collapse_mentions)\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(infomation[2], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's tokenize all tweets.\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=False, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True)\n",
      "              for t in info1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Store these in a sparse matrix.\n",
      "\n",
      "#1) Create a vocabulary (dict from term->index)\n",
      "\n",
      "# https://docs.python.org/2/library/collections.html#collections.defaultdict\n",
      "from collections import defaultdict\n",
      "\n",
      "def make_vocabulary(tokens_list):\n",
      "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
      "    for tokens in tokens_list:\n",
      "        for token in tokens:\n",
      "            vocabulary[token]\n",
      "    print '%d unique terms in vocabulary' % len(vocabulary)\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "111835 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert features to a sparse matrix X.\n",
      "# X[i,j] is the frequency of term j in tweet i\n",
      "# \n",
      "from scipy.sparse import lil_matrix\n",
      "\n",
      "def make_feature_matrix(tokens_list, vocabulary):\n",
      "    X = lil_matrix((len(info), len(vocabulary)))\n",
      "    for i, tokens in enumerate(tokens_list):\n",
      "        for token in tokens:\n",
      "            j = vocabulary[token]\n",
      "            X[i,j] += 1\n",
      "    return X.tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = make_feature_matrix(tokens_list, vocabulary)\n",
      "print 'shape of X:', X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shape of X: (692520, 111835)\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute z = X * \\beta, where X is a CSR matrix.\n",
      "import numpy as np\n",
      "beta = np.ones(len(vocabulary))  # assume Beta = vector of 1s\n",
      "z = np.zeros(len(info))\n",
      "for i in range(len(info)):  # for each row.\n",
      "    for j in range(X.indptr[i], X.indptr[i+1]): # for each col.\n",
      "        colidx = X.indices[j]\n",
      "        z[i] += beta[colidx] * X.data[j]\n",
      "print z[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 13.  11.  18.  29.  34.]\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# y is a 1d numpy array of gender labels.\n",
      "# Let 1=Female, 0=Male.\n",
      "import numpy as np\n",
      "\n",
      "def get_gender(t, male_names, female_names):\n",
      "    name_label = get_first_name(t)\n",
      "    if name_label in female_names:\n",
      "        return 1\n",
      "    elif name_label in male_names:\n",
      "        return 0\n",
      "    else:\n",
      "        return -1\n",
      "    \n",
      "y = np.array([get_gender(t, male_names, female_names) for t in real_name])\n",
      "print 'gender labels:', Counter(y).items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gender labels: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 131151), (1, 199145)]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Do 5-fold cross-validation\n",
      "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val(X, y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.790000621514\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How does tokenization affect accuracy?\n",
      "# Collapse urls and mentions; ignore description prefix.\n",
      "def run_all(info, use_descr=True, lowercase=True,\n",
      "            keep_punctuation=True, descr_prefix=None,\n",
      "            collapse_urls=True, collapse_mentions=True):\n",
      "    \n",
      "    tokens_list = [tweet2tokens(t, use_descr, lowercase,\n",
      "                            keep_punctuation, descr_prefix,\n",
      "                            collapse_urls, collapse_mentions)\n",
      "                  for t in info]\n",
      "    vocabulary = make_vocabulary(tokens_list)\n",
      "    X = make_feature_matrix(tokens_list, vocabulary)\n",
      "    print 'acc=', do_cross_val(X, y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tweet2tokens(info, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    tokens = []\n",
      "    if use_text:\n",
      "        tokens.extend(tokenize(info[1], lowercase, keep_punctuation, None,\n",
      "                           collapse_urls, collapse_mentions))\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(info[2], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_all(info, use_descr=True, lowercase=True,\n",
      "            keep_punctuation=True, descr_prefix=None,\n",
      "            collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    \n",
      "    tokens_list = [tweet2tokens(t, use_descr, lowercase,\n",
      "                            keep_punctuation, descr_prefix,\n",
      "                            collapse_urls, collapse_mentions, use_text)\n",
      "                  for t in info]\n",
      "    vocabulary = make_vocabulary(tokens_list)\n",
      "    X = make_feature_matrix(tokens_list, vocabulary)\n",
      "    acc = do_cross_val(X, y, 5)\n",
      "    print 'acc=', acc\n",
      "    return acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_all(info1, use_descr=True, use_text=True)\n",
      "run_all(info1, use_descr=True, use_text=False)\n",
      "run_all(info1, use_descr=False, use_text=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "215094 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.80339462956\n",
        "146124 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.822416945902\n",
        "91978 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.593927822218\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "0.59392782221758778"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tokens_list[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['that', 'just', 'threw', 'me', 'off', 'i', 'm', 'so', 'done', 'thewalkingdead', 'd=lol', 'd=hi', 'd=playstationnation', 'd=thecrewtubers', 'd=tbh', 'd=you', 'd=should', 'd=follow', 'd=me', 'd=i', 'd=follow', 'd=back', 'd=c', 'd=kbaithxs'], ['oh', 'snap', 'that', 'ending', 'though', 'thewalkingdead', 'd=shar', 'd=here', 'd=an', 'd=anthro', 'd=furry', 'd=artist', 'd=from', 'd=fa', 'd=posting', 'd=random', 'd=stuff', 'd=occasionally', 'd=nsfw', 'd=p'], ['rt', 'THIS_IS_A_MENTION', 'last', 'night', 's', 'thewalkingdead', 'still', 'resonates', 'horror', 'in', 'my', 'thoughts', 'today', 'a', 'wicked', 'disturbing', 'cringe', 'worthy', 'episode', 'ht', 'd=the', 'd=walking', 'd=dead', 'd=video', 'd=games', 'd=chandler', 'd=riggs', 'd=that', 'd=stuff', 'd=and', 'd=thangs']]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}